{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pytorch_lightning as pl\n",
    "import os\n",
    "import pickle\n",
    "from typing import List, Tuple, Dict\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch_geometric\n",
    "from joblib import Parallel, delayed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the model\n",
    "class PopulationGraphModel(pl.LightningModule):\n",
    "    def __init__(self, num_nodes, embedding_dim=256):  # Fixed parameter definition\n",
    "        super().__init__()\n",
    "        \n",
    "        # Save parameters\n",
    "        self.num_nodes = num_nodes\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        # Node embedding layer\n",
    "        self.node_embedding = nn.Embedding(num_nodes, embedding_dim)\n",
    "        \n",
    "        # GNN layers\n",
    "        self.conv1 = GCNConv(embedding_dim, 512)\n",
    "        self.conv2 = GCNConv(512, embedding_dim)\n",
    "        \n",
    "        ## Set up model parameters\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(embedding_dim * 2, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Metrics\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.train_f1s = []\n",
    "        self.val_f1s = []\n",
    "        \n",
    "    def forward(self, x1: torch.Tensor, x2: torch.Tensor, edge_index: torch.Tensor = None) -> torch.Tensor:\n",
    "        \n",
    "        if edge_index is not None:\n",
    "            # Apply GNN layers if we have graph structure\n",
    "            x = self.node_embedding(torch.arange(self.hparams.num_nodes).to(x1.device))\n",
    "            x = self.conv1(x, edge_index).relu()\n",
    "            x = self.conv2(x, edge_index)\n",
    "            \n",
    "            # Get relevant node embeddings\n",
    "            x1_emb = x[x1]\n",
    "            x2_emb = x[x2]\n",
    "        else:\n",
    "            # Direct embedding lookup if no graph structure\n",
    "            x1_emb = self.node_embedding(x1)\n",
    "            x2_emb = self.node_embedding(x2)\n",
    "        \n",
    "        # Combine embeddings\n",
    "        combined = torch.cat([x1_emb, x2_emb], dim=1)\n",
    "        features = self.encoder(combined)\n",
    "        return self.classifier(features)\n",
    "    \n",
    "    def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor, torch.Tensor], \n",
    "                     batch_idx: int) -> torch.Tensor:\n",
    "        x1, x2, y = batch\n",
    "        y_hat = self(x1, x2)\n",
    "        loss = nn.BCELoss()(y_hat, y)\n",
    "        \n",
    "        # Calculate F1 score\n",
    "        with torch.no_grad():\n",
    "            predictions = (y_hat > 0.5).float()\n",
    "            f1 = f1_score(y.cpu().numpy(), predictions.cpu().numpy())\n",
    "        \n",
    "        # Log metrics\n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        self.log('train_f1', f1, prog_bar=True)\n",
    "        self.train_losses.append(loss.item())\n",
    "        self.train_f1s.append(f1)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor, torch.Tensor], \n",
    "                       batch_idx: int) -> Dict:\n",
    "        x1, x2, y = batch\n",
    "        y_hat = self(x1, x2)\n",
    "        loss = nn.BCELoss()(y_hat, y)\n",
    "        \n",
    "        # Calculate F1 score\n",
    "        predictions = (y_hat > 0.5).float()\n",
    "        f1 = f1_score(y.cpu().numpy(), predictions.cpu().numpy())\n",
    "        \n",
    "        # Log metrics\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        self.log('val_f1', f1, prog_bar=True)\n",
    "        self.val_losses.append(loss.item())\n",
    "        self.val_f1s.append(f1)\n",
    "        \n",
    "        return {'val_loss': loss, 'val_f1': f1}\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            mode='min',\n",
    "            factor=0.5,\n",
    "            patience=10,\n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': scheduler,\n",
    "            'monitor': 'val_loss'\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(patient_pheno_lists: List, \n",
    "                val_split: float = 0.2) -> Tuple[TensorDataset, TensorDataset]:\n",
    "    \"\"\"\n",
    "    Prepare training and validation datasets\n",
    "    \"\"\"\n",
    "    n_patients = len(patient_pheno_lists)\n",
    "    \n",
    "    # Create pairs and labels\n",
    "    pairs = []\n",
    "    labels = []\n",
    "    for i in range(n_patients):\n",
    "        for j in range(i+1, min(i+100, n_patients)):\n",
    "            pairs.append((i, j))\n",
    "            shared = len(set(patient_pheno_lists[i]).intersection(\n",
    "                set(patient_pheno_lists[j]))) > 0\n",
    "            labels.append(float(shared))\n",
    "    \n",
    "    pairs = torch.tensor(pairs)\n",
    "    labels = torch.tensor(labels)\n",
    "    \n",
    "    # Split train/val\n",
    "    n_samples = len(pairs)\n",
    "    n_val = int(n_samples * val_split)\n",
    "    indices = torch.randperm(n_samples)\n",
    "    \n",
    "    train_indices = indices[n_val:]\n",
    "    val_indices = indices[:n_val]\n",
    "    \n",
    "    # Create datasets\n",
    "    train_data = TensorDataset(\n",
    "        pairs[train_indices, 0],\n",
    "        pairs[train_indices, 1],\n",
    "        labels[train_indices].unsqueeze(1)\n",
    "    )\n",
    "    \n",
    "    val_data = TensorDataset(\n",
    "        pairs[val_indices, 0],\n",
    "        pairs[val_indices, 1],\n",
    "        labels[val_indices].unsqueeze(1)\n",
    "    )\n",
    "    \n",
    "    logger.info(f\"Created {len(train_data)} training samples and {len(val_data)} validation samples\")\n",
    "    return train_data, val_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(trainer: pl.Trainer, save_dir: str = './outputs'):\n",
    "    \"\"\"\n",
    "    Plot training metrics\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Plot losses\n",
    "    plt.subplot(121)\n",
    "    plt.plot(trainer.model.train_losses, label='Train Loss')\n",
    "    plt.plot(trainer.model.val_losses, label='Val Loss')\n",
    "    plt.title('Loss Over Time')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot F1 scores\n",
    "    plt.subplot(122)\n",
    "    plt.plot(trainer.model.train_f1s, label='Train F1')\n",
    "    plt.plot(trainer.model.val_f1s, label='Val F1')\n",
    "    plt.title('F1 Score Over Time')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, 'training_metrics.png'))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(num_nodes: int,\n",
    "               patient_pheno_lists: List,\n",
    "               save_dir: str = './outputs',\n",
    "               max_epochs: int = 100) -> Tuple[PopulationGraphModel, pl.Trainer]:\n",
    "    \"\"\"\n",
    "    Train the population graph model\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Prepare data\n",
    "    train_data, val_data = prepare_data(patient_pheno_lists)\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(train_data, batch_size=4096, shuffle=True, num_workers=4)\n",
    "    val_loader = DataLoader(val_data, batch_size=4096, num_workers=4)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = PopulationGraphModel(num_nodes=num_nodes)\n",
    "    \n",
    "    # Initialize trainer\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=max_epochs,\n",
    "        accelerator='gpu',\n",
    "        callbacks=[\n",
    "            pl.callbacks.EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=20,\n",
    "                mode='min'\n",
    "            ),\n",
    "            pl.callbacks.ModelCheckpoint(\n",
    "                dirpath=save_dir,\n",
    "                filename='best_model',\n",
    "                monitor='val_loss',\n",
    "                mode='min'\n",
    "            )\n",
    "        ],\n",
    "        logger=True\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "    \n",
    "    # Plot and save metrics\n",
    "    plot_metrics(trainer, save_dir)\n",
    "    \n",
    "    return model, trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(i, batch_start, batch_end, model, device):\n",
    "    \"\"\"Process a batch of adjacency matrix rows in parallel.\"\"\"\n",
    "    pairs_i = torch.tensor([i] * (batch_end - batch_start)).to(device)\n",
    "    pairs_j = torch.arange(batch_start, batch_end).to(device)\n",
    "    preds = model(pairs_i, pairs_j).cpu()  # Move results back to CPU to save memory\n",
    "    results = []\n",
    "    for idx, k in enumerate(range(batch_start, batch_end)):\n",
    "        results.append((i, k, preds[idx].item()))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_adjacency_matrix_parallel(model, patient_pheno_lists, batch_size=1000, num_jobs=-1, save_path=None):\n",
    "    \"\"\"\n",
    "    Generate adjacency matrix for a population graph using parallel processing.\n",
    "\n",
    "    Args:\n",
    "        model: Trained PyTorch model for inference.\n",
    "        patient_pheno_lists: List of patient phenotypes.\n",
    "        batch_size: Number of samples to process in a batch.\n",
    "        num_jobs: Number of parallel jobs for processing (-1 uses all available cores).\n",
    "        save_path: Path to save the adjacency matrix incrementally (optional).\n",
    "\n",
    "    Returns:\n",
    "        adj_matrix: Generated adjacency matrix as a torch tensor.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device).eval()  # Move model to device and set to eval mode\n",
    "\n",
    "    n_patients = len(patient_pheno_lists)\n",
    "    adj_matrix = torch.zeros((n_patients, n_patients))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(n_patients), desc=\"Processing patients\"):\n",
    "            # Use joblib to parallelize the batch processing\n",
    "            results = Parallel(n_jobs=num_jobs)(\n",
    "                delayed(process_batch)(i, j, min(j + batch_size, n_patients), model, device)\n",
    "                for j in range(i + 1, n_patients, batch_size)\n",
    "            )\n",
    "\n",
    "            # Update adjacency matrix from results\n",
    "            for batch_results in results:\n",
    "                for i, k, value in batch_results:\n",
    "                    adj_matrix[i, k] = value\n",
    "                    adj_matrix[k, i] = value\n",
    "\n",
    "            # Save progress incrementally if a save_path is provided\n",
    "            if save_path and i % 100 == 0:\n",
    "                torch.save(adj_matrix, save_path)\n",
    "\n",
    "    # Save the final matrix if save_path is provided\n",
    "    if save_path:\n",
    "        torch.save(adj_matrix, save_path)\n",
    "\n",
    "    return adj_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_adjacency_matrix(model: PopulationGraphModel,\n",
    "                            patient_pheno_lists: List) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Generate adjacency matrix for population graph\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        n_patients = len(patient_pheno_lists)\n",
    "        adj_matrix = torch.zeros((n_patients, n_patients))\n",
    "        \n",
    "        # Process in batches for memory efficiency\n",
    "        batch_size = 100000\n",
    "        for i in tqdm(range(n_patients)):\n",
    "            for j in range(i+1, n_patients, batch_size):\n",
    "                batch_end = min(j + batch_size, n_patients)\n",
    "                \n",
    "                # Create pairs for this batch\n",
    "                pairs_i = torch.tensor(list(range(i, i+1))).repeat(batch_end - j)\n",
    "                pairs_j = torch.tensor(list(range(j, batch_end)))\n",
    "                \n",
    "                # Get predictions\n",
    "                preds = model(pairs_i, pairs_j)\n",
    "                \n",
    "                # Fill adjacency matrix\n",
    "                for idx, k in enumerate(range(j, batch_end)):\n",
    "                    adj_matrix[i,k] = preds[idx]\n",
    "                    adj_matrix[k,i] = preds[idx]\n",
    "    \n",
    "    return adj_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max node index: 70272\n",
      "Number of nodes: 70273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Created 2864981 training samples and 716245 validation samples\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "/home/kai/anaconda3/lib/python3.11/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /kai/Kai_Backup/Study/GiG in rare diease detection/outputs exists and is not empty.\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/kai/anaconda3/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "INFO:pytorch_lightning.callbacks.model_summary:\n",
      "  | Name           | Type       | Params | Mode \n",
      "------------------------------------------------------\n",
      "0 | node_embedding | Embedding  | 18.0 M | train\n",
      "1 | conv1          | GCNConv    | 131 K  | train\n",
      "2 | conv2          | GCNConv    | 131 K  | train\n",
      "3 | encoder        | Sequential | 395 K  | train\n",
      "4 | classifier     | Sequential | 33.3 K | train\n",
      "------------------------------------------------------\n",
      "18.7 M    Trainable params\n",
      "0         Non-trainable params\n",
      "18.7 M    Total params\n",
      "74.726    Total estimated model params size (MB)\n",
      "23        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e77b12a88a7d44618beff3e57d62ec43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc6eb4bf9d2f413d9ee2a2bcdbf8ba4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5bbdaf251404c9ea4f6afbf258e445b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b64dc20cd434b1fabd9e62273edfe88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9f45c2c2aaf473b88fbf4b63faf2e15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "065513bd88a34513ae4aa8f674b8571a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41f70df0ea8a4ed8a01052a9d667c0ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67ac197bb6294d808e915a5725947dff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "962ffafc1d594a99b186dabcd2752819",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75ea70c5601048568980306b68439c0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deeccf677fa3441788414c7e039c081d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e33df627767d4a96811f7a5dd455d7cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4b1da07bcd34316ae2b0305c1271245",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec7c02117f17411baeb6b7c27be29170",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb7ee38609f745e3b0e95bff8dee488d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4d42cadcb1f48fdb21fece7bd40895f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86c51c8888eb4ce1852ffbc7c3089c76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c88175cb4aa149b187b28fde644e539c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72a644dc904d447c8329a80c0d5fc9c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "670bd79ea7ca4eca86baca77a3aff1a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aeb4ee2c6238463180e280a2ceae2d86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f6d0b9a99bc492b9b62584d3af31c61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e03e59f0723494da988eb3103739120",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c12b66da00964a70aa1e0ab0dffa4530",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09f48d18008145d5a846852dbc3e50cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e234f8e7bad4f8d939bb5817e6e4b8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be694521fbe44ccdaa33456f6c63c595",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2156235f08084f74b5b3a012e603712d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54e13fde924e4c0b806ce592a2c79bee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fd5811b4a824876bd2c38729d7cbe3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36224/36224 [3:15:13<00:00,  3.09it/s]  \n",
      "100%|██████████| 6400/6400 [06:08<00:00, 17.38it/s] \n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Set random seeds\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Create output directory\n",
    "    output_dir = './Output'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Load preprocessed data\n",
    "    with open(f'{output_dir}/train_patients_phenotypes_list.pkl', 'rb') as f:\n",
    "        train_patients_phenotypes_list = pickle.load(f)\n",
    "        \n",
    "    with open(f'{output_dir}/val_patients_phenotypes_list.pkl', 'rb') as f:\n",
    "        val_patients_phenotypes_list = pickle.load(f)\n",
    "    \n",
    "    # Get max node index instead of counting unique phenotypes\n",
    "    max_node_idx = max(\n",
    "        max(max(phenos) if phenos else 0 for phenos in train_patients_phenotypes_list),\n",
    "        max(max(phenos) if phenos else 0 for phenos in val_patients_phenotypes_list)\n",
    "    )\n",
    "    num_nodes = max_node_idx + 1  # Add 1 because indices are 0-based\n",
    "    \n",
    "    print(f\"Max node index: {max_node_idx}\")\n",
    "    print(f\"Number of nodes: {num_nodes}\")\n",
    "    \n",
    "    # Train model with correct number of nodes\n",
    "    model, trainer = train_model(num_nodes, train_patients_phenotypes_list)\n",
    "    \n",
    "    # Save model\n",
    "    torch.save(model.state_dict(), os.path.join(output_dir, 'final_model.pt'))\n",
    "    \n",
    "    # Generate and save adjacency matrices for train and val sets\n",
    "    train_adj_matrix = generate_adjacency_matrix(model, train_patients_phenotypes_list)\n",
    "    val_adj_matrix = generate_adjacency_matrix(model, val_patients_phenotypes_list)\n",
    "    \n",
    "    torch.save(train_adj_matrix, os.path.join(output_dir, 'train_adjacency_matrix.pt'))\n",
    "    torch.save(val_adj_matrix, os.path.join(output_dir, 'val_adjacency_matrix.pt'))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_568956/4102984715.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000e+00, 2.4411e-01, 3.8471e-01,  ..., 8.0281e-01, 2.0837e-04,\n",
      "         1.1505e-04],\n",
      "        [2.4411e-01, 0.0000e+00, 1.3909e-02,  ..., 3.6500e-01, 9.9997e-01,\n",
      "         9.9706e-01],\n",
      "        [3.8471e-01, 1.3909e-02, 0.0000e+00,  ..., 1.1961e-03, 1.4693e-04,\n",
      "         1.4688e-04],\n",
      "        ...,\n",
      "        [8.0281e-01, 3.6500e-01, 1.1961e-03,  ..., 0.0000e+00, 1.8188e-03,\n",
      "         3.3910e-03],\n",
      "        [2.0837e-04, 9.9997e-01, 1.4693e-04,  ..., 1.8188e-03, 0.0000e+00,\n",
      "         9.7018e-01],\n",
      "        [1.1505e-04, 9.9706e-01, 1.4688e-04,  ..., 3.3910e-03, 9.7018e-01,\n",
      "         0.0000e+00]])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "file_path = './Output/train_adjacency_matrix.pt'\n",
    "data = torch.load(file_path)\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
