{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import torch \n",
    "\n",
    "import pickle\n",
    "from kg import KnowledgeGraph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.environ.get('DATA_DIR', './Data')\n",
    "output_dir = os.environ.get('OUTPUT_DIR', './Output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Add node indices\n",
    "# ## nodes_df: Dataframe containing node information with columns: node_idx, node_id, node_type\n",
    "# ## relations_df: Dataframe containing relation information with columns: source ID/type, target ID/type\n",
    "\n",
    "# def add_node_indices(nodes_df, relations_df):\n",
    "#     \"\"\"\n",
    "#     Add node indices from nodes dataframe to relations dataframe based on ID and type matching.\n",
    "    \n",
    "#     Args:\n",
    "#     \tnodes_df (pd.DataFrame): Dataframe containing node information with columns: node_idx, node_id, node_type\n",
    "#     \trelations_df (pd.DataFrame): Dataframe containing relation information with columns: x_id, x_type, y_id, y_type\n",
    "   \n",
    "#     Returns:\n",
    "#     \tpd.DataFrame: Filtered relations dataframe with added x_idx, y_idx, and included columns\n",
    "#     \"\"\"\n",
    "#     # Create a copy of the relations dataframe to avoid modifying the original\n",
    "#     result_df = relations_df.copy()\n",
    "    \n",
    "#     # Convert node_id to string type in nodes_df for consistent matching\n",
    "#     nodes_df = nodes_df.copy()\n",
    "#     nodes_df['node_id'] = nodes_df['node_id'].astype(str)\n",
    "    \n",
    "#     # Convert x_id and y_id to string type in relations_df\n",
    "#     result_df['x_id'] = result_df['x_id'].astype(str)\n",
    "#     result_df['y_id'] = result_df['y_id'].astype(str)\n",
    "    \n",
    "#     # Create a mapping dictionary from (node_id, node_type) to node_idx\n",
    "#     node_mapping = pd.Series(\n",
    "#         nodes_df.node_idx.values,\n",
    "#         index=pd.MultiIndex.from_arrays([nodes_df.node_id, nodes_df.node_type])\n",
    "#     ).to_dict()\n",
    "    \n",
    "#     # Create a function to safely get mapping value\n",
    "#     def get_mapping(row, id_col, type_col):\n",
    "#         key = (row[id_col], row[type_col])\n",
    "#         return node_mapping.get(key)\n",
    "    \n",
    "#     # Add x_idx column by mapping (x_id, x_type)\n",
    "#     result_df['x_idx'] = result_df.apply(\n",
    "#         lambda row: get_mapping(row, 'x_id', 'x_type'), \n",
    "#         axis=1\n",
    "#     )\n",
    "    \n",
    "#     # Add y_idx column by mapping (y_id, y_type)\n",
    "#     result_df['y_idx'] = result_df.apply(\n",
    "#         lambda row: get_mapping(row, 'y_id', 'y_type'), \n",
    "#         axis=1\n",
    "#     )\n",
    "    \n",
    "#     # Check if both indices are valid (not NaN) and add 'included' column\n",
    "#     result_df['included'] = result_df['x_idx'].notna() & result_df['y_idx'].notna()\n",
    "    \n",
    "#     # Filter rows where both x_idx and y_idx are not NaN\n",
    "#     result_df = result_df[result_df['included']].copy()\n",
    "    \n",
    "#     # Convert x_idx and y_idx to integers\n",
    "#     result_df['x_idx'] = result_df['x_idx'].astype(int)\n",
    "#     result_df['y_idx'] = result_df['y_idx'].astype(int)\n",
    "    \n",
    "#     return result_df\n",
    "\n",
    "\n",
    "# data_dir = os.environ.get('DATA_DIR', './Data')\n",
    "# output_dir = os.environ.get('OUTPUT_DIR', './Output')\n",
    "\n",
    "# df = pd.read_csv(f'{data_dir}/knowledge_graph/8.9.21_kg/kg_giant_orphanet.csv')\n",
    "# print(\"Loading HPO to index dictionary...\")\n",
    "\n",
    "# with open(f'{data_dir}/knowledge_graph/8.9.21_kg/hpo_to_idx_dict_8.9.21_kg.pkl', 'rb') as f:\n",
    "#     hpo_to_idx_dict = pickle.load(f)\n",
    "\n",
    "# print(\"Loading Genes to index dictionary...\")\n",
    "# with open(f'{data_dir}/knowledge_graph/8.9.21_kg/ensembl_to_idx_dict_8.9.21_kg.pkl', 'rb') as file:\n",
    "#     genes = pickle.load(file)\n",
    "\n",
    "# with open(f'{data_dir}/preprocess/orphanet/orphanet_to_mondo_dict.pkl', 'rb') as file:\n",
    "#         orphanet_to_mondo_dict = pickle.load(file)\n",
    "\n",
    "# with open(f'{data_dir}/knowledge_graph/8.9.21_kg/mondo_to_idx_dict_8.9.21_kg.pkl', 'rb') as file:\n",
    "#     mondo_to_idx_dict = pickle.load(file)\n",
    "\n",
    "# # create_nodes_pkl(data_dir)\n",
    "# with open(f'{data_dir}/knowledge_graph/8.9.21_kg/nodes_8.9.21_kg.pkl', 'rb') as file:\n",
    "#     nodes = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # The nodes_8.9.21_kg.pkl file contains node information with columns: node_idx, node_id, node_type\n",
    "# with open(f'{data_dir}/knowledge_graph/8.9.21_kg/nodes_8.9.21_kg.pkl', 'rb') as file:\n",
    "#     nodes = pickle.load(file)\n",
    "\n",
    "# print(\"\\nFirst 5 rows of the data:\")\n",
    "# print(nodes.head(5),\"\\n\")\n",
    "# print(\"there are:\",nodes.shape[0],\"of nodes \\n\")\n",
    "# print(nodes['node_type'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # the kg_giant_orphanet.csv file contains the relationships between nodes with columns\n",
    "# # this is the predefined relationships between nodes\n",
    "# df = pd.read_csv(f'{data_dir}/knowledge_graph/8.9.21_kg/kg_giant_orphanet.csv')\n",
    "\n",
    "# print(\"\\nFirst 5 rows of relationships:\")\n",
    "# print(df.head(5), \"\\n\")\n",
    "# print(\"there are:\",df.shape[0],\"of relationships \\n\")\n",
    "# print(df['relation'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # View some HPO indices\n",
    "# print(\"Sample HPO indices:\")\n",
    "# print(list(hpo_to_idx_dict.items())[:5])\n",
    "\n",
    "# # View some gene indices\n",
    "# print(\"\\nSample gene indices:\")\n",
    "# print(list(genes.items())[:5])\n",
    "\n",
    "# # View some disease indices\n",
    "# print(\"\\nSample disease indices:\")\n",
    "# print(list(mondo_to_idx_dict.items())[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## enriched information about the nodes and their relationships\n",
    "# ## output: result.csv, after mapping the relationship into nodes, we built the knowledge graph\n",
    "# result = add_node_indices(nodes, df)\n",
    "# result.to_csv(f'{output_dir}/result.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Generate and construct the knowledge graph\n",
    "\n",
    "# ## Initialize KG\n",
    "# kg = KnowledgeGraph()\n",
    "\n",
    "# ## Chucksize is the size of knowledge graph data to be loaded at once\n",
    "# kg.create_from_csv(\n",
    "#     filepath=f'./{output_dir}/result.csv',\n",
    "#     nodes_filepath = f'{data_dir}/knowledge_graph/8.9.21_kg/nodes_8.9.21_kg.pkl',\n",
    "#     chunksize=105220,  # Adjust based on your memory constraints\n",
    "#     show_progress=True\n",
    "# )\n",
    "# # Save the graph\n",
    "# ## This is the knowledge graph\n",
    "# kg.save_graph(f'{output_dir}/my_graph_ids.graphml')\n",
    "\n",
    "\n",
    "# # # code to double check the KG \n",
    "# # subgraph = kg.plot_random_subgraph(num_nodes=10000, method='random_walk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # # Initialize KG\n",
    "# kg = KnowledgeGraph()\n",
    "# kg.load_graph(f'{output_dir}/my_graph_ids.graphml') #!!!!!!! if u want proper plotting u need to save and then load the graph, not work directly after creation the kg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # WORKING with PATINIET DATA\n",
    "# print('Loading and processing patient data...')\n",
    "# file_path = f'{data_dir}/patients/simulated_patients/disease_split_val_sim_patients_8.9.21_kg.txt'  # val data!!!!\n",
    "# with open(file_path, 'r') as f:\n",
    "#     val_data = [json.loads(line) for line in f]\n",
    "\n",
    "# file_path = f'{data_dir}/patients/simulated_patients/disease_split_train_sim_patients_8.9.21_kg.txt'  # train data!!!!\n",
    "# with open(file_path, 'r') as f:\n",
    "#     train_data = [json.loads(line) for line in f]\n",
    "\n",
    "\n",
    "# ## Map the phenotype to the index\n",
    "# val_patients_phenotypes_list = [[hpo_to_idx_dict[phenotype] for phenotype in p['positive_phenotypes']] for p in val_data]\n",
    "# train_patients_phenotypes_list = [[hpo_to_idx_dict[phenotype] for phenotype in p['positive_phenotypes']] for p in train_data]\n",
    "\n",
    "\n",
    "# # Save the list to a pickle file\n",
    "# with open(f'{output_dir}/val_patients_phenotypes_list.pkl', 'wb') as f:\n",
    "#     pickle.dump(val_patients_phenotypes_list, f)\n",
    "\n",
    "# ## Map genes to index\n",
    "# val_patients_genes = [\n",
    "#     [genes[g] for g in p['true_genes']]\n",
    "#     for p in val_data\n",
    "# ]\n",
    "\n",
    "\n",
    "# # WORKING with PATINIET DATA\n",
    "# print('Loading and processing patient data...')\n",
    "# file_path = f'{data_dir}/patients/simulated_patients/disease_split_val_sim_patients_8.9.21_kg.txt'  # val data!!!!\n",
    "# with open(file_path, 'r') as f:\n",
    "#     val_data = [json.loads(line) for line in f]\n",
    "\n",
    "# file_path = f'{data_dir}/patients/simulated_patients/disease_split_train_sim_patients_8.9.21_kg.txt'  # train data!!!!\n",
    "# with open(file_path, 'r') as f:\n",
    "#     train_data = [json.loads(line) for line in f]\n",
    "\n",
    "\n",
    "# ## Map the phenotype to the index\n",
    "# val_patients_phenotypes_list = [[hpo_to_idx_dict[phenotype] for phenotype in p['positive_phenotypes']] for p in val_data]\n",
    "# train_patients_phenotypes_list = [[hpo_to_idx_dict[phenotype] for phenotype in p['positive_phenotypes']] for p in train_data]\n",
    "\n",
    "\n",
    "# # Save the list to a pickle file\n",
    "# with open(f'{output_dir}/val_patients_phenotypes_list.pkl', 'wb') as f:\n",
    "#     pickle.dump(val_patients_phenotypes_list, f)\n",
    "\n",
    "# ## Map genes to index\n",
    "# val_patients_genes = [[genes[g] for g in p['true_genes']] for p in val_data]\n",
    "# train_patients_genes = [[genes[g] for g in p['true_genes']] for p in train_data]\n",
    "\n",
    "# # Save the list to a pickle file\n",
    "# with open(f'{output_dir}/train_patients_phenotypes_list.pkl', 'wb') as f:\n",
    "#     pickle.dump(train_patients_phenotypes_list, f)\n",
    "\n",
    "\n",
    "# # Save the list to a pickle file\n",
    "# with open(f'{output_dir}/val_patients_genes.pkl', 'wb') as f:\n",
    "#     pickle.dump(val_patients_genes, f)\n",
    "\n",
    "# # Save the list to a pickle file\n",
    "# with open(f'{output_dir}/train_patients_genes.pkl', 'wb') as f:\n",
    "#     pickle.dump(train_patients_genes, f)\n",
    "\n",
    "# ## Map the disease to the index\n",
    "# val_all_candidate_genes = [[genes[g] for g in p['all_candidate_genes'] if g in genes] for p in val_data]\n",
    "# train_all_candidate_genes = [[genes[g] for g in p['all_candidate_genes'] if g in genes] for p in train_data]\n",
    "\n",
    "# # Save the list to a pickle file\n",
    "# with open(f'{output_dir}/val_all_candidate_genes.pkl', 'wb') as f:\n",
    "#     pickle.dump(val_all_candidate_genes, f)\n",
    "\n",
    "# # Save the list to a pickle file\n",
    "# with open(f'{output_dir}/train_all_candidate_genes.pkl', 'wb') as f:\n",
    "#     pickle.dump(train_all_candidate_genes, f)\n",
    "\n",
    "# print(\"Saved patient_training and validation data\")\n",
    "\n",
    "# ## process test data\n",
    "# file_path = f'{data_dir}/patients/mygene2_patients/mygene2_5.7.22.txt'  # val data!!!!\n",
    "\n",
    "# with open(file_path, 'r') as f:\n",
    "#     test_data = [json.loads(line) for line in f]\n",
    "    \n",
    "# test_patients_phenotypes_list = [[hpo_to_idx_dict[phenotype] for phenotype in p['positive_phenotypes']] for p in test_data]\n",
    "\n",
    "\n",
    "# # Save the list to a pickle file\n",
    "# with open(f'{output_dir}/test_patients_phenotypes_list.pkl', 'wb') as f:\n",
    "#     pickle.dump(test_patients_phenotypes_list, f)\n",
    "\n",
    "# test_patients_genes = [[genes[g] for g in p['true_genes']] for p in test_data]\n",
    "\n",
    "# # Save the list to a pickle file\n",
    "# with open(f'{output_dir}/test_patients_genes.pkl', 'wb') as f:\n",
    "#     pickle.dump(test_patients_genes, f)\n",
    "\n",
    "\n",
    "# test_all_candidate_genes = [[genes[g] for g in p['all_candidate_genes'] if g in genes] for p in test_data]\n",
    "\n",
    "# # Save the list to a pickle file\n",
    "# with open(f'{output_dir}/test_all_candidate_genes.pkl', 'wb') as f:\n",
    "#     pickle.dump(test_all_candidate_genes, f)\n",
    "\n",
    "# print(\"Saved patient_test data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # What in training data \n",
    "# # True genes that cause specific diseases\n",
    "# # True symptoms\n",
    "# # True age of conset\n",
    "# # All candidate genes\n",
    "\n",
    "# train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Plot out the test data\n",
    "# ## The grpah shows the connection between the patient's phenotypes, the candidate genes and the true genes of test data\n",
    "\n",
    "# i=1\n",
    "\n",
    "# kg.visualize_subgraph([str(n) for n in test_patients_phenotypes_list[i] + test_all_candidate_genes[i] + test_patients_genes[i]], \n",
    "#                                 true_gene_ids=[str(k) for k in test_patients_genes[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## To map the local id to global id\n",
    "# train_nx_subgraph = []\n",
    "# train_pg_subgraph = []\n",
    "\n",
    "# for i in range(len(train_patients_phenotypes_list)):\n",
    "#     subgraph = kg.create_subgraph([str(n) for n in train_patients_phenotypes_list[i] + train_all_candidate_genes[i] + train_patients_genes[i]], \n",
    "#                                 true_gene_ids=[str(k) for k in train_patients_genes[i]])\n",
    "#     pg_graph = kg.create_pyg_data_from_subgraph(subgraph)\n",
    "#     train_nx_subgraph.append(subgraph)\n",
    "#     train_pg_subgraph.append(pg_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_pg_subgraph[0])\n",
    "# ## x = 58 nodes, there are 7 types of nodes\n",
    "# ## edge_index: 196 connections between nodes\n",
    "# ## edge_attr: 15 types of the connections\n",
    "# ## There are 36224 patients in the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_nx_subgraph = []\n",
    "# val_pg_subgraph = []\n",
    "\n",
    "# for i in range(len(val_patients_phenotypes_list)):\n",
    "\n",
    "#     # Without patients\n",
    "#     subgraph = kg.create_subgraph([str(n) for n in val_patients_phenotypes_list[i]+ val_all_candidate_genes[i]+ val_patients_genes[i]], true_gene_ids = [str(k) for k in val_patients_genes[i]])\n",
    "#     pg_graph = kg.create_pyg_data_from_subgraph(subgraph)\n",
    "#     val_nx_subgraph.append(subgraph)\n",
    "#     val_pg_subgraph.append(pg_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_pg_subgraph[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_nx_subgraph = []\n",
    "# test_pg_subgraph = []\n",
    "\n",
    "# for i in range(len(test_patients_phenotypes_list)):\n",
    "#     subgraph = kg.create_subgraph([str(n) for n in test_patients_phenotypes_list[i] + test_all_candidate_genes[i] + test_patients_genes[i]], \n",
    "#                                 true_gene_ids=[str(k) for k in test_patients_genes[i]])\n",
    "#     pg_graph = kg.create_pyg_data_from_subgraph(subgraph)\n",
    "#     test_nx_subgraph.append(subgraph)\n",
    "#     test_pg_subgraph.append(pg_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def save_data(graphs, save_dir):\n",
    "#     os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "#     for i, graph in enumerate(graphs):\n",
    "#         torch.save(graph, f'{save_dir}/graph_{i}.pt')\n",
    "\n",
    "# save_dir = \"/kai/Kai_Backup/Study/GiG in rare diease detection/Data/saved_graphs/test\"\n",
    "# save_data(test_pg_subgraph, save_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_54356/4275355714.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graphs.append(torch.load(f'{save_dir}/{file}'))\n"
     ]
    }
   ],
   "source": [
    "def load_data(save_dir):\n",
    "    graphs = []\n",
    "    for file in os.listdir(save_dir):\n",
    "        if file.endswith(\".pt\"):\n",
    "            graphs.append(torch.load(f'{save_dir}/{file}'))\n",
    "    return graphs\n",
    "\n",
    "save_dir = \"./Data/saved_graphs/train\"\n",
    "train_pg_subgraph = load_data(save_dir)\n",
    "\n",
    "save_dir = \"./Data/saved_graphs/val\"\n",
    "val_pg_subgraph = load_data(save_dir)\n",
    "\n",
    "save_dir = \"./Data/saved_graphs/test\"\n",
    "test_pg_subgraph = load_data(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of unique true gene ids is: 2405\n",
      "{4: 0, 10: 1, 8204: 2, 8208: 3, 18: 4, 8210: 5, 22: 6, 23: 7, 29: 8, 30: 9, 8224: 10, 33: 11, 37: 12, 8231: 13, 41: 14, 8234: 15, 43: 16, 42: 17, 8246: 18, 61: 19, 67: 20, 8266: 21, 74: 22, 8268: 23, 78: 24, 80: 25, 83: 26, 85: 27, 87: 28, 88: 29, 8281: 30, 8283: 31, 8284: 32, 92: 33, 95: 34, 97: 35, 98: 36, 100: 37, 111: 38, 8305: 39, 8318: 40, 130: 41, 133: 42, 143: 43, 144: 44, 146: 45, 150: 46, 8345: 47, 154: 48, 57506: 49, 163: 50, 162: 51, 57507: 52, 169: 53, 8362: 54, 173: 55, 177: 56, 178: 57, 8371: 58, 8372: 59, 57521: 60, 8376: 61, 8378: 62, 8379: 63, 187: 64, 8388: 65, 8395: 66, 207: 67, 209: 68, 212: 69, 8405: 70, 213: 71, 8411: 72, 8416: 73, 8422: 74, 232: 75, 8428: 76, 238: 77, 242: 78, 8434: 79, 8437: 80, 246: 81, 8439: 82, 245: 83, 249: 84, 57595: 85, 8446: 86, 256: 87, 8454: 88, 8456: 89, 8458: 90, 8462: 91, 8464: 92, 57618: 93, 8466: 94, 276: 95, 279: 96, 8473: 97, 8477: 98, 8480: 99, 288: 100, 57632: 101, 291: 102, 8484: 103, 8483: 104, 8486: 105, 8487: 106, 296: 107, 8489: 108, 298: 109, 8491: 110, 8485: 111, 297: 112, 8494: 113, 303: 114, 57651: 115, 8501: 116, 8505: 117, 8506: 118, 315: 119, 8514: 120, 8515: 121, 8526: 122, 8527: 123, 337: 124, 339: 125, 341: 126, 344: 127, 349: 128, 8543: 129, 352: 130, 362: 131, 363: 132, 8555: 133, 8566: 134, 375: 135, 8570: 136, 8571: 137, 383: 138, 8578: 139, 387: 140, 8580: 141, 388: 142, 390: 143, 391: 144, 393: 145, 395: 146, 8589: 147, 398: 148, 405: 149, 407: 150, 410: 151, 8604: 152, 8605: 153, 8606: 154, 420: 155, 426: 156, 8620: 157, 429: 158, 428: 159, 431: 160, 440: 161, 8644: 162, 8646: 163, 455: 164, 460: 165, 8656: 166, 8658: 167, 8660: 168, 470: 169, 474: 170, 476: 171, 477: 172, 8671: 173, 481: 174, 487: 175, 489: 176, 490: 177, 492: 178, 8687: 179, 496: 180, 501: 181, 504: 182, 505: 183, 509: 184, 8702: 185, 8709: 186, 8712: 187, 522: 188, 526: 189, 8719: 190, 8725: 191, 535: 192, 540: 193, 543: 194, 544: 195, 8737: 196, 553: 197, 8746: 198, 8747: 199, 554: 200, 560: 201, 8754: 202, 8764: 203, 574: 204, 576: 205, 577: 206, 582: 207, 8775: 208, 591: 209, 594: 210, 602: 211, 603: 212, 608: 213, 8801: 214, 611: 215, 8805: 216, 8806: 217, 8810: 218, 8817: 219, 625: 220, 8819: 221, 628: 222, 8821: 223, 8823: 224, 8824: 225, 642: 226, 643: 227, 656: 228, 8854: 229, 8855: 230, 665: 231, 666: 232, 668: 233, 8860: 234, 8864: 235, 8869: 236, 679: 237, 680: 238, 8871: 239, 682: 240, 8875: 241, 691: 242, 8885: 243, 695: 244, 702: 245, 708: 246, 711: 247, 8905: 248, 714: 249, 715: 250, 8908: 251, 8906: 252, 721: 253, 8919: 254, 727: 255, 729: 256, 735: 257, 742: 258, 743: 259, 8936: 260, 751: 261, 754: 262, 8946: 263, 756: 264, 762: 265, 769: 266, 775: 267, 780: 268, 789: 269, 790: 270, 8983: 271, 791: 272, 8986: 273, 8987: 274, 794: 275, 795: 276, 802: 277, 803: 278, 8995: 279, 9001: 280, 9005: 281, 9010: 282, 9020: 283, 9024: 284, 9025: 285, 58178: 286, 839: 287, 841: 288, 847: 289, 852: 290, 855: 291, 857: 292, 858: 293, 862: 294, 863: 295, 867: 296, 9064: 297, 874: 298, 9066: 299, 9069: 300, 878: 301, 880: 302, 9073: 303, 9079: 304, 888: 305, 9081: 306, 891: 307, 9091: 308, 906: 309, 909: 310, 9106: 311, 916: 312, 919: 313, 9116: 314, 927: 315, 929: 316, 934: 317, 935: 318, 940: 319, 9135: 320, 9137: 321, 948: 322, 955: 323, 9149: 324, 9152: 325, 9154: 326, 9155: 327, 9161: 328, 9165: 329, 981: 330, 988: 331, 989: 332, 9184: 333, 995: 334, 996: 335, 58342: 336, 1000: 337, 58348: 338, 1013: 339, 1014: 340, 9210: 341, 1020: 342, 9216: 343, 9219: 344, 9220: 345, 1029: 346, 58375: 347, 9228: 348, 1040: 349, 9233: 350, 9237: 351, 1046: 352, 9238: 353, 1049: 354, 9244: 355, 9246: 356, 1057: 357, 9256: 358, 1066: 359, 1069: 360, 1072: 361, 1073: 362, 9264: 363, 9268: 364, 1082: 365, 1085: 366, 1086: 367, 9280: 368, 9282: 369, 1093: 370, 9286: 371, 1098: 372, 9299: 373, 9300: 374, 1109: 375, 9306: 376, 1116: 377, 9309: 378, 1120: 379, 9312: 380, 1124: 381, 9316: 382, 1126: 383, 1130: 384, 1131: 385, 1132: 386, 9323: 387, 1135: 388, 9330: 389, 1148: 390, 9341: 391, 9343: 392, 1151: 393, 1153: 394, 1154: 395, 9345: 396, 9350: 397, 9352: 398, 9353: 399, 1164: 400, 1165: 401, 1173: 402, 1175: 403, 9370: 404, 9371: 405, 1181: 406, 1188: 407, 9382: 408, 1193: 409, 9388: 410, 1198: 411, 9391: 412, 1204: 413, 9397: 414, 1205: 415, 1210: 416, 9403: 417, 9408: 418, 1216: 419, 9410: 420, 1221: 421, 1222: 422, 1224: 423, 1229: 424, 9424: 425, 1235: 426, 1239: 427, 9435: 428, 9438: 429, 9441: 430, 1251: 431, 1253: 432, 1255: 433, 1263: 434, 9459: 435, 1270: 436, 9466: 437, 9469: 438, 9471: 439, 9475: 440, 9487: 441, 1296: 442, 9489: 443, 1298: 444, 1300: 445, 1301: 446, 1309: 447, 1311: 448, 1312: 449, 1314: 450, 1340: 451, 1344: 452, 1350: 453, 1362: 454, 1365: 455, 9558: 456, 1367: 457, 1366: 458, 9562: 459, 1371: 460, 9563: 461, 1373: 462, 1374: 463, 1378: 464, 1379: 465, 1383: 466, 1389: 467, 9589: 468, 9604: 469, 1414: 470, 9613: 471, 9631: 472, 1448: 473, 1455: 474, 9652: 475, 9655: 476, 1464: 477, 1466: 478, 1471: 479, 9667: 480, 1476: 481, 9673: 482, 9676: 483, 1485: 484, 9683: 485, 1493: 486, 1495: 487, 9691: 488, 1500: 489, 9695: 490, 1515: 491, 9711: 492, 1520: 493, 9714: 494, 1523: 495, 1522: 496, 1526: 497, 1527: 498, 9720: 499, 9721: 500, 9718: 501, 1531: 502, 9724: 503, 1535: 504, 1537: 505, 1553: 506, 9746: 507, 9747: 508, 9748: 509, 1557: 510, 1558: 511, 1559: 512, 9754: 513, 1564: 514, 9767: 515, 9778: 516, 1587: 517, 1593: 518, 1600: 519, 1604: 520, 1606: 521, 9800: 522, 1611: 523, 9807: 524, 1618: 525, 9813: 526, 9814: 527, 1622: 528, 1626: 529, 9824: 530, 1633: 531, 9826: 532, 1632: 533, 9829: 534, 9831: 535, 1639: 536, 1641: 537, 9833: 538, 9835: 539, 1640: 540, 1645: 541, 1648: 542, 1650: 543, 1651: 544, 1653: 545, 9855: 546, 1666: 547, 1682: 548, 9875: 549, 1684: 550, 1685: 551, 1687: 552, 1691: 553, 1692: 554, 9891: 555, 1706: 556, 1707: 557, 1709: 558, 1710: 559, 9901: 560, 9905: 561, 1715: 562, 8482: 563, 1721: 564, 1722: 565, 9915: 566, 1724: 567, 1725: 568, 1738: 569, 1739: 570, 9933: 571, 1742: 572, 1741: 573, 1744: 574, 1746: 575, 9938: 576, 1748: 577, 1756: 578, 1757: 579, 1760: 580, 1761: 581, 1762: 582, 9954: 583, 9956: 584, 9965: 585, 1774: 586, 9967: 587, 1775: 588, 9970: 589, 1781: 590, 9975: 591, 1785: 592, 9978: 593, 9986: 594, 9987: 595, 9988: 596, 1807: 597, 1811: 598, 1825: 599, 10018: 600, 1831: 601, 10027: 602, 10032: 603, 10034: 604, 10037: 605, 10041: 606, 1852: 607, 1863: 608, 1866: 609, 1868: 610, 1873: 611, 1874: 612, 1876: 613, 10074: 614, 10075: 615, 1885: 616, 1887: 617, 10084: 618, 1894: 619, 1897: 620, 1899: 621, 10093: 622, 10094: 623, 1902: 624, 1904: 625, 1905: 626, 1910: 627, 1920: 628, 1921: 629, 1928: 630, 1931: 631, 10123: 632, 1934: 633, 1935: 634, 1939: 635, 10132: 636, 1942: 637, 10144: 638, 1955: 639, 1961: 640, 1968: 641, 10161: 642, 1974: 643, 10167: 644, 10169: 645, 1978: 646, 1977: 647, 1980: 648, 1983: 649, 10176: 650, 1987: 651, 10180: 652, 10181: 653, 10183: 654, 1995: 655, 1996: 656, 10197: 657, 2005: 658, 10198: 659, 2017: 660, 2024: 661, 2027: 662, 10223: 663, 2032: 664, 10225: 665, 2037: 666, 10236: 667, 10241: 668, 2055: 669, 2057: 670, 10250: 671, 2059: 672, 10253: 673, 10260: 674, 2069: 675, 2070: 676, 10263: 677, 10265: 678, 2075: 679, 10270: 680, 2083: 681, 10277: 682, 10278: 683, 2087: 684, 10282: 685, 2092: 686, 2096: 687, 10289: 688, 2098: 689, 2099: 690, 10290: 691, 10293: 692, 10295: 693, 2103: 694, 2104: 695, 10299: 696, 2108: 697, 10300: 698, 10303: 699, 2112: 700, 2111: 701, 2117: 702, 2120: 703, 2123: 704, 2128: 705, 2129: 706, 2132: 707, 2134: 708, 2137: 709, 10329: 710, 2140: 711, 2148: 712, 2157: 713, 2158: 714, 2160: 715, 10352: 716, 2164: 717, 10356: 718, 10358: 719, 70444: 720, 2169: 721, 2174: 722, 2175: 723, 10367: 724, 10370: 725, 10371: 726, 10372: 727, 2181: 728, 2182: 729, 10385: 730, 10386: 731, 2198: 732, 2199: 733, 2205: 734, 2213: 735, 2217: 736, 10413: 737, 2221: 738, 10416: 739, 10417: 740, 2228: 741, 10422: 742, 2231: 743, 10424: 744, 2234: 745, 10428: 746, 10429: 747, 10431: 748, 2244: 749, 2249: 750, 70461: 751, 2267: 752, 10459: 753, 10464: 754, 10465: 755, 2274: 756, 2276: 757, 2278: 758, 2286: 759, 2289: 760, 2290: 761, 10485: 762, 2298: 763, 10492: 764, 10495: 765, 2308: 766, 2310: 767, 2311: 768, 2314: 769, 2316: 770, 2320: 771, 2324: 772, 10517: 773, 10521: 774, 2330: 775, 2331: 776, 2332: 777, 10524: 778, 2337: 779, 10531: 780, 2341: 781, 2345: 782, 2347: 783, 2348: 784, 10542: 785, 10546: 786, 10550: 787, 2361: 788, 10559: 789, 10563: 790, 10566: 791, 2382: 792, 2383: 793, 2389: 794, 10582: 795, 2399: 796, 2400: 797, 10593: 798, 10594: 799, 10597: 800, 2406: 801, 2408: 802, 10602: 803, 2411: 804, 2417: 805, 10613: 806, 2423: 807, 2427: 808, 2432: 809, 2434: 810, 2448: 811, 2452: 812, 2453: 813, 10647: 814, 10649: 815, 10654: 816, 2465: 817, 10664: 818, 2473: 819, 2475: 820, 10667: 821, 2481: 822, 2491: 823, 2497: 824, 2498: 825, 10691: 826, 2500: 827, 2501: 828, 10695: 829, 10696: 830, 2504: 831, 2505: 832, 10699: 833, 2509: 834, 2511: 835, 2515: 836, 2526: 837, 10725: 838, 2542: 839, 2543: 840, 10736: 841, 2545: 842, 10740: 843, 10754: 844, 10756: 845, 2569: 846, 2572: 847, 10769: 848, 2580: 849, 2581: 850, 10774: 851, 2585: 852, 2587: 853, 2593: 854, 10788: 855, 10798: 856, 10802: 857, 2616: 858, 2621: 859, 2622: 860, 10818: 861, 2630: 862, 2631: 863, 10824: 864, 2633: 865, 10827: 866, 59982: 867, 2639: 868, 10836: 869, 10837: 870, 2646: 871, 10847: 872, 2663: 873, 2664: 874, 2666: 875, 2667: 876, 2671: 877, 10865: 878, 2674: 879, 10867: 880, 2673: 881, 2677: 882, 2679: 883, 10873: 884, 10874: 885, 10878: 886, 2693: 887, 2698: 888, 2699: 889, 2700: 890, 10894: 891, 60049: 892, 10900: 893, 60052: 894, 2710: 895, 2711: 896, 10902: 897, 2714: 898, 2716: 899, 2719: 900, 51878: 901, 10923: 902, 51885: 903, 10930: 904, 2742: 905, 2743: 906, 2744: 907, 60087: 908, 2746: 909, 51897: 910, 51894: 911, 51902: 912, 60095: 913, 10944: 914, 51903: 915, 2754: 916, 10946: 917, 2756: 918, 60099: 919, 2759: 920, 2767: 921, 2768: 922, 2788: 923, 51945: 924, 2794: 925, 51947: 926, 10988: 927, 51949: 928, 51948: 929, 2799: 930, 2795: 931, 10985: 932, 2803: 933, 11002: 934, 2810: 935, 51965: 936, 2814: 937, 11008: 938, 2817: 939, 2821: 940, 2824: 941, 11030: 942, 2840: 943, 11034: 944, 51997: 945, 11038: 946, 52002: 947, 2853: 948, 11046: 949, 52007: 950, 52008: 951, 11051: 952, 2860: 953, 2859: 954, 52015: 955, 11055: 956, 11057: 957, 2864: 958, 52020: 959, 2873: 960, 52026: 961, 52029: 962, 11076: 963, 2889: 964, 2893: 965, 2895: 966, 52048: 967, 52047: 968, 2898: 969, 2903: 970, 52056: 971, 2905: 972, 2907: 973, 11101: 974, 11104: 975, 2914: 976, 11107: 977, 2916: 978, 52069: 979, 52073: 980, 11114: 981, 52076: 982, 2927: 983, 11120: 984, 11121: 985, 11124: 986, 2933: 987, 2936: 988, 11130: 989, 11134: 990, 2942: 991, 2943: 992, 52096: 993, 11136: 994, 11135: 995, 52097: 996, 52094: 997, 2963: 998, 11156: 999, 11155: 1000, 2970: 1001, 52125: 1002, 52127: 1003, 11172: 1004, 2984: 1005, 52139: 1006, 11180: 1007, 2993: 1008, 11192: 1009, 11193: 1010, 3000: 1011, 52162: 1012, 3011: 1013, 3012: 1014, 11207: 1015, 11209: 1016, 52174: 1017, 11214: 1018, 3025: 1019, 11220: 1020, 3032: 1021, 3035: 1022, 3036: 1023, 11229: 1024, 3038: 1025, 52193: 1026, 3042: 1027, 3043: 1028, 3045: 1029, 52197: 1030, 11238: 1031, 3047: 1032, 52203: 1033, 3052: 1034, 3053: 1035, 11247: 1036, 3056: 1037, 3057: 1038, 3065: 1039, 3073: 1040, 11270: 1041, 11273: 1042, 3085: 1043, 3088: 1044, 52241: 1045, 11283: 1046, 11286: 1047, 3099: 1048, 52254: 1049, 3104: 1050, 3106: 1051, 52260: 1052, 3109: 1053, 3113: 1054, 52265: 1055, 11307: 1056, 3132: 1057, 3133: 1058, 11324: 1059, 52288: 1060, 52290: 1061, 11330: 1062, 52295: 1063, 52296: 1064, 52297: 1065, 11339: 1066, 11341: 1067, 3151: 1068, 52305: 1069, 11356: 1070, 11357: 1071, 52318: 1072, 3170: 1073, 3172: 1074, 3175: 1075, 3176: 1076, 11369: 1077, 52330: 1078, 3179: 1079, 3181: 1080, 3182: 1081, 11376: 1082, 52348: 1083, 3200: 1084, 11394: 1085, 3207: 1086, 52360: 1087, 52362: 1088, 3212: 1089, 3216: 1090, 52374: 1091, 11423: 1092, 11425: 1093, 3235: 1094, 52390: 1095, 3239: 1096, 11431: 1097, 3242: 1098, 3246: 1099, 52399: 1100, 11440: 1101, 11443: 1102, 52405: 1103, 11452: 1104, 11458: 1105, 52418: 1106, 11460: 1107, 3275: 1108, 11469: 1109, 3277: 1110, 3279: 1111, 52436: 1112, 11478: 1113, 3288: 1114, 11481: 1115, 11484: 1116, 11485: 1117, 3294: 1118, 3296: 1119, 3301: 1120, 11497: 1121, 3307: 1122, 11504: 1123, 52466: 1124, 3314: 1125, 52472: 1126, 3321: 1127, 52477: 1128, 3326: 1129, 11521: 1130, 3330: 1131, 11523: 1132, 3331: 1133, 11525: 1134, 52485: 1135, 3335: 1136, 3337: 1137, 3338: 1138, 3347: 1139, 11539: 1140, 11545: 1141, 3355: 1142, 52512: 1143, 11554: 1144, 11555: 1145, 11561: 1146, 52524: 1147, 52527: 1148, 3380: 1149, 3382: 1150, 3384: 1151, 52536: 1152, 11580: 1153, 11585: 1154, 11592: 1155, 52553: 1156, 3402: 1157, 11596: 1158, 52559: 1159, 11600: 1160, 3409: 1161, 52562: 1162, 3416: 1163, 3421: 1164, 3424: 1165, 11621: 1166, 3430: 1167, 3431: 1168, 11625: 1169, 11627: 1170, 3435: 1171, 11633: 1172, 3442: 1173, 3446: 1174, 11642: 1175, 11643: 1176, 11652: 1177, 3471: 1178, 3474: 1179, 3476: 1180, 11670: 1181, 52632: 1182, 3484: 1183, 11681: 1184, 3491: 1185, 3493: 1186, 3500: 1187, 3501: 1188, 3504: 1189, 56056: 1190, 3506: 1191, 52663: 1192, 11708: 1193, 52670: 1194, 11718: 1195, 11719: 1196, 11721: 1197, 11722: 1198, 3529: 1199, 11727: 1200, 11741: 1201, 52702: 1202, 3553: 1203, 3556: 1204, 3557: 1205, 52712: 1206, 3565: 1207, 3567: 1208, 52720: 1209, 3578: 1210, 52731: 1211, 3588: 1212, 3590: 1213, 3592: 1214, 52748: 1215, 11789: 1216, 3598: 1217, 3600: 1218, 11797: 1219, 3607: 1220, 11800: 1221, 52763: 1222, 11811: 1223, 11813: 1224, 3622: 1225, 52775: 1226, 3625: 1227, 3627: 1228, 52781: 1229, 11823: 1230, 3635: 1231, 3638: 1232, 3641: 1233, 52793: 1234, 3645: 1235, 11839: 1236, 11844: 1237, 11846: 1238, 11857: 1239, 11860: 1240, 3672: 1241, 11865: 1242, 3675: 1243, 3679: 1244, 52834: 1245, 3683: 1246, 11879: 1247, 3690: 1248, 52844: 1249, 3693: 1250, 3694: 1251, 11886: 1252, 52850: 1253, 11891: 1254, 3698: 1255, 3702: 1256, 3707: 1257, 52861: 1258, 3709: 1259, 3713: 1260, 3720: 1261, 3721: 1262, 3722: 1263, 11919: 1264, 3728: 1265, 3735: 1266, 52891: 1267, 3741: 1268, 3749: 1269, 52903: 1270, 11944: 1271, 3753: 1272, 3755: 1273, 3756: 1274, 11951: 1275, 3759: 1276, 11955: 1277, 3765: 1278, 3772: 1279, 3773: 1280, 3775: 1281, 52928: 1282, 11976: 1283, 11984: 1284, 3804: 1285, 11999: 1286, 12002: 1287, 52965: 1288, 3815: 1289, 52970: 1290, 3819: 1291, 52979: 1292, 12020: 1293, 3830: 1294, 12029: 1295, 3840: 1296, 53003: 1297, 12050: 1298, 3862: 1299, 3863: 1300, 3864: 1301, 12055: 1302, 12059: 1303, 53020: 1304, 12066: 1305, 3875: 1306, 3878: 1307, 3879: 1308, 12076: 1309, 3886: 1310, 12079: 1311, 3894: 1312, 53052: 1313, 3906: 1314, 3911: 1315, 3916: 1316, 3917: 1317, 53072: 1318, 12112: 1319, 12113: 1320, 3923: 1321, 12116: 1322, 3925: 1323, 53078: 1324, 3929: 1325, 12121: 1326, 3937: 1327, 3938: 1328, 3941: 1329, 53098: 1330, 3947: 1331, 12140: 1332, 53101: 1333, 12150: 1334, 53112: 1335, 12154: 1336, 3967: 1337, 3971: 1338, 12163: 1339, 3972: 1340, 53127: 1341, 3976: 1342, 3980: 1343, 12174: 1344, 53135: 1345, 3988: 1346, 3989: 1347, 53144: 1348, 53146: 1349, 3995: 1350, 12189: 1351, 3999: 1352, 4003: 1353, 12199: 1354, 53161: 1355, 4010: 1356, 4015: 1357, 4016: 1358, 12207: 1359, 4018: 1360, 4019: 1361, 53172: 1362, 4022: 1363, 4023: 1364, 4025: 1365, 12223: 1366, 4033: 1367, 4037: 1368, 12237: 1369, 12238: 1370, 4048: 1371, 53201: 1372, 53202: 1373, 12240: 1374, 12244: 1375, 4051: 1376, 4054: 1377, 4058: 1378, 53211: 1379, 12256: 1380, 4067: 1381, 12260: 1382, 53221: 1383, 4071: 1384, 4080: 1385, 4083: 1386, 12276: 1387, 12277: 1388, 4092: 1389, 4094: 1390, 12289: 1391, 53251: 1392, 4105: 1393, 4107: 1394, 4112: 1395, 12305: 1396, 12304: 1397, 4114: 1398, 53268: 1399, 12307: 1400, 53289: 1401, 12332: 1402, 12334: 1403, 53294: 1404, 12343: 1405, 12354: 1406, 4164: 1407, 53318: 1408, 53320: 1409, 12361: 1410, 4169: 1411, 4172: 1412, 4173: 1413, 4176: 1414, 12371: 1415, 4180: 1416, 4182: 1417, 4184: 1418, 4187: 1419, 53341: 1420, 12383: 1421, 12385: 1422, 4194: 1423, 4200: 1424, 4206: 1425, 4209: 1426, 4210: 1427, 4213: 1428, 4215: 1429, 12409: 1430, 4221: 1431, 4224: 1432, 53379: 1433, 4233: 1434, 4237: 1435, 4244: 1436, 4249: 1437, 12441: 1438, 4258: 1439, 53411: 1440, 4260: 1441, 4259: 1442, 12450: 1443, 4264: 1444, 4266: 1445, 4271: 1446, 12467: 1447, 4277: 1448, 12474: 1449, 4288: 1450, 12483: 1451, 53444: 1452, 4294: 1453, 4296: 1454, 12493: 1455, 4316: 1456, 4321: 1457, 53477: 1458, 12518: 1459, 4325: 1460, 12520: 1461, 12522: 1462, 12525: 1463, 4338: 1464, 4339: 1465, 53495: 1466, 4344: 1467, 12537: 1468, 12538: 1469, 4349: 1470, 4352: 1471, 12548: 1472, 12549: 1473, 53509: 1474, 12554: 1475, 4366: 1476, 12558: 1477, 4371: 1478, 4373: 1479, 4376: 1480, 12572: 1481, 12573: 1482, 12575: 1483, 4386: 1484, 4387: 1485, 4389: 1486, 4393: 1487, 4394: 1488, 12588: 1489, 4397: 1490, 12594: 1491, 12599: 1492, 12607: 1493, 4416: 1494, 4421: 1495, 4422: 1496, 4426: 1497, 4427: 1498, 4433: 1499, 4435: 1500, 12629: 1501, 4441: 1502, 53597: 1503, 4447: 1504, 4453: 1505, 12646: 1506, 4455: 1507, 12650: 1508, 4463: 1509, 12656: 1510, 4466: 1511, 4469: 1512, 12664: 1513, 4473: 1514, 53628: 1515, 4485: 1516, 4486: 1517, 4489: 1518, 12681: 1519, 4490: 1520, 12686: 1521, 4495: 1522, 53648: 1523, 53651: 1524, 53652: 1525, 12695: 1526, 12699: 1527, 4511: 1528, 53665: 1529, 12706: 1530, 12721: 1531, 4532: 1532, 53686: 1533, 12727: 1534, 4536: 1535, 12729: 1536, 12733: 1537, 4545: 1538, 4550: 1539, 53704: 1540, 4556: 1541, 4557: 1542, 12769: 1543, 4579: 1544, 12774: 1545, 53734: 1546, 53736: 1547, 4585: 1548, 12777: 1549, 53741: 1550, 53742: 1551, 12783: 1552, 53748: 1553, 12792: 1554, 4600: 1555, 4615: 1556, 4616: 1557, 12809: 1558, 4621: 1559, 12815: 1560, 4623: 1561, 53780: 1562, 53782: 1563, 12823: 1564, 4633: 1565, 4637: 1566, 4640: 1567, 4653: 1568, 4655: 1569, 4656: 1570, 12850: 1571, 4662: 1572, 4663: 1573, 4666: 1574, 12858: 1575, 12861: 1576, 12868: 1577, 12869: 1578, 4676: 1579, 12873: 1580, 4682: 1581, 12876: 1582, 4688: 1583, 12880: 1584, 12888: 1585, 53850: 1586, 53858: 1587, 53860: 1588, 4710: 1589, 53865: 1590, 53869: 1591, 4721: 1592, 12917: 1593, 4727: 1594, 4730: 1595, 12923: 1596, 12925: 1597, 4734: 1598, 12927: 1599, 53889: 1600, 12936: 1601, 12937: 1602, 53898: 1603, 4752: 1604, 12945: 1605, 12951: 1606, 70301: 1607, 70302: 1608, 4767: 1609, 4769: 1610, 4778: 1611, 4782: 1612, 53935: 1613, 12980: 1614, 4789: 1615, 4796: 1616, 70334: 1617, 70335: 1618, 70336: 1619, 4802: 1620, 12997: 1621, 70342: 1622, 4809: 1623, 4811: 1624, 13004: 1625, 4813: 1626, 53966: 1627, 4812: 1628, 4817: 1629, 70355: 1630, 70356: 1631, 13012: 1632, 13014: 1633, 53979: 1634, 70363: 1635, 70365: 1636, 4838: 1637, 70375: 1638, 70376: 1639, 70378: 1640, 70379: 1641, 13036: 1642, 70381: 1643, 53996: 1644, 70380: 1645, 13040: 1646, 70385: 1647, 70389: 1648, 13046: 1649, 4858: 1650, 54011: 1651, 54013: 1652, 70398: 1653, 4862: 1654, 54015: 1655, 70406: 1656, 4871: 1657, 13068: 1658, 54030: 1659, 13070: 1660, 4880: 1661, 13079: 1662, 70424: 1663, 4889: 1664, 13081: 1665, 4890: 1666, 54047: 1667, 70431: 1668, 70433: 1669, 70437: 1670, 4902: 1671, 13097: 1672, 70441: 1673, 4905: 1674, 4908: 1675, 70445: 1676, 4910: 1677, 70442: 1678, 13099: 1679, 13105: 1680, 70450: 1681, 70451: 1682, 70452: 1683, 70453: 1684, 70454: 1685, 70449: 1686, 70448: 1687, 70457: 1688, 70458: 1689, 13110: 1690, 54077: 1691, 70462: 1692, 4925: 1693, 70464: 1694, 13121: 1695, 13122: 1696, 70466: 1697, 70467: 1698, 70465: 1699, 4933: 1700, 70463: 1701, 13135: 1702, 4944: 1703, 4949: 1704, 13142: 1705, 4951: 1706, 54111: 1707, 4960: 1708, 13154: 1709, 13162: 1710, 4975: 1711, 4979: 1712, 4984: 1713, 4986: 1714, 4989: 1715, 4990: 1716, 4991: 1717, 4992: 1718, 4995: 1719, 13188: 1720, 4999: 1721, 5001: 1722, 5002: 1723, 5005: 1724, 5008: 1725, 13202: 1726, 13206: 1727, 5030: 1728, 5032: 1729, 13229: 1730, 13230: 1731, 5042: 1732, 5048: 1733, 13243: 1734, 5052: 1735, 5054: 1736, 13247: 1737, 5056: 1738, 105211: 1739, 13256: 1740, 13257: 1741, 13264: 1742, 5074: 1743, 13267: 1744, 5077: 1745, 13269: 1746, 13271: 1747, 5080: 1748, 13277: 1749, 5099: 1750, 5101: 1751, 54256: 1752, 5104: 1753, 54259: 1754, 13301: 1755, 13302: 1756, 54261: 1757, 13304: 1758, 5129: 1759, 13325: 1760, 5141: 1761, 5144: 1762, 5151: 1763, 13346: 1764, 5158: 1765, 5162: 1766, 5169: 1767, 13375: 1768, 5185: 1769, 5190: 1770, 5191: 1771, 13383: 1772, 5193: 1773, 5199: 1774, 5202: 1775, 13394: 1776, 13397: 1777, 13401: 1778, 13409: 1779, 5227: 1780, 54385: 1781, 54386: 1782, 13427: 1783, 5238: 1784, 5255: 1785, 13452: 1786, 5264: 1787, 13457: 1788, 54418: 1789, 5268: 1790, 5270: 1791, 13464: 1792, 54425: 1793, 54432: 1794, 5281: 1795, 5286: 1796, 13482: 1797, 13483: 1798, 5294: 1799, 5299: 1800, 5301: 1801, 13498: 1802, 5307: 1803, 5311: 1804, 13513: 1805, 13515: 1806, 5324: 1807, 5325: 1808, 13516: 1809, 13517: 1810, 5331: 1811, 5337: 1812, 5339: 1813, 13531: 1814, 5340: 1815, 5343: 1816, 5344: 1817, 5345: 1818, 5352: 1819, 5357: 1820, 13550: 1821, 5359: 1822, 54511: 1823, 5362: 1824, 5367: 1825, 54520: 1826, 5369: 1827, 13559: 1828, 5368: 1829, 54525: 1830, 13568: 1831, 54533: 1832, 13579: 1833, 13585: 1834, 13586: 1835, 5397: 1836, 54549: 1837, 5406: 1838, 5407: 1839, 5416: 1840, 5417: 1841, 54571: 1842, 5420: 1843, 13615: 1844, 54575: 1845, 5431: 1846, 5432: 1847, 5433: 1848, 5436: 1849, 5437: 1850, 5443: 1851, 13640: 1852, 5450: 1853, 13643: 1854, 54609: 1855, 13651: 1856, 5465: 1857, 5478: 1858, 5479: 1859, 5489: 1860, 13684: 1861, 5499: 1862, 5501: 1863, 13698: 1864, 13699: 1865, 5513: 1866, 13706: 1867, 5521: 1868, 5524: 1869, 13718: 1870, 5530: 1871, 5535: 1872, 13727: 1873, 5538: 1874, 13731: 1875, 13734: 1876, 5544: 1877, 13739: 1878, 13740: 1879, 5550: 1880, 5551: 1881, 5552: 1882, 13745: 1883, 13746: 1884, 13748: 1885, 13758: 1886, 5571: 1887, 13767: 1888, 13768: 1889, 5586: 1890, 5589: 1891, 5590: 1892, 13784: 1893, 5594: 1894, 5597: 1895, 5599: 1896, 5603: 1897, 5608: 1898, 13802: 1899, 5617: 1900, 5624: 1901, 5626: 1902, 5630: 1903, 5638: 1904, 5643: 1905, 5647: 1906, 5649: 1907, 13842: 1908, 13845: 1909, 13846: 1910, 5654: 1911, 5659: 1912, 5670: 1913, 5671: 1914, 5672: 1915, 5674: 1916, 5678: 1917, 54831: 1918, 13872: 1919, 5682: 1920, 54838: 1921, 54839: 1922, 13884: 1923, 13885: 1924, 54845: 1925, 5695: 1926, 13891: 1927, 54852: 1928, 5713: 1929, 5716: 1930, 54870: 1931, 5719: 1932, 5720: 1933, 5726: 1934, 5728: 1935, 5730: 1936, 5731: 1937, 13923: 1938, 5733: 1939, 13928: 1940, 54890: 1941, 5743: 1942, 5744: 1943, 13937: 1944, 5749: 1945, 54912: 1946, 13959: 1947, 5767: 1948, 5769: 1949, 54925: 1950, 5774: 1951, 5776: 1952, 5784: 1953, 5794: 1954, 5806: 1955, 5809: 1956, 5812: 1957, 5813: 1958, 14008: 1959, 5817: 1960, 5823: 1961, 54976: 1962, 5825: 1963, 5826: 1964, 54981: 1965, 5829: 1966, 5834: 1967, 5848: 1968, 55000: 1969, 5853: 1970, 5862: 1971, 5866: 1972, 5869: 1973, 5877: 1974, 55030: 1975, 5879: 1976, 55048: 1977, 5897: 1978, 5899: 1979, 5911: 1980, 5912: 1981, 5921: 1982, 55076: 1983, 5952: 1984, 5959: 1985, 5963: 1986, 5965: 1987, 5972: 1988, 5984: 1989, 5985: 1990, 5998: 1991, 6002: 1992, 6004: 1993, 6010: 1994, 6011: 1995, 6018: 1996, 6019: 1997, 6028: 1998, 6033: 1999, 6061: 2000, 6066: 2001, 6067: 2002, 6068: 2003, 6073: 2004, 6081: 2005, 6088: 2006, 6097: 2007, 6110: 2008, 6116: 2009, 6121: 2010, 6122: 2011, 6137: 2012, 6140: 2013, 6144: 2014, 6149: 2015, 6156: 2016, 6160: 2017, 6163: 2018, 6167: 2019, 6173: 2020, 6177: 2021, 6182: 2022, 6186: 2023, 6187: 2024, 6193: 2025, 6194: 2026, 6206: 2027, 6221: 2028, 6229: 2029, 6232: 2030, 6233: 2031, 6234: 2032, 6236: 2033, 6250: 2034, 6253: 2035, 6254: 2036, 6258: 2037, 6262: 2038, 6269: 2039, 6283: 2040, 6284: 2041, 6288: 2042, 6289: 2043, 6290: 2044, 6296: 2045, 6299: 2046, 6303: 2047, 6308: 2048, 6312: 2049, 6314: 2050, 6315: 2051, 6349: 2052, 6350: 2053, 6357: 2054, 6361: 2055, 6367: 2056, 6373: 2057, 6380: 2058, 6386: 2059, 55538: 2060, 6399: 2061, 6400: 2062, 55551: 2063, 6408: 2064, 6409: 2065, 6410: 2066, 6415: 2067, 6417: 2068, 6420: 2069, 55584: 2070, 6432: 2071, 6435: 2072, 6440: 2073, 6442: 2074, 6454: 2075, 6458: 2076, 6460: 2077, 6466: 2078, 6468: 2079, 6484: 2080, 6487: 2081, 6491: 2082, 6499: 2083, 6501: 2084, 6504: 2085, 6506: 2086, 55677: 2087, 6528: 2088, 55682: 2089, 6532: 2090, 55688: 2091, 6540: 2092, 55698: 2093, 6558: 2094, 55714: 2095, 55715: 2096, 6567: 2097, 6570: 2098, 55723: 2099, 55731: 2100, 6590: 2101, 6594: 2102, 6598: 2103, 6601: 2104, 55756: 2105, 55757: 2106, 6606: 2107, 6607: 2108, 55765: 2109, 55778: 2110, 55779: 2111, 6636: 2112, 6637: 2113, 6640: 2114, 6642: 2115, 6655: 2116, 6662: 2117, 6664: 2118, 6669: 2119, 6676: 2120, 6683: 2121, 6686: 2122, 6690: 2123, 6691: 2124, 6694: 2125, 6715: 2126, 6716: 2127, 6719: 2128, 6721: 2129, 6723: 2130, 6737: 2131, 6739: 2132, 55897: 2133, 6753: 2134, 6757: 2135, 6763: 2136, 55918: 2137, 6782: 2138, 6784: 2139, 6801: 2140, 6804: 2141, 6808: 2142, 6809: 2143, 55963: 2144, 6815: 2145, 6826: 2146, 6840: 2147, 6842: 2148, 6846: 2149, 6847: 2150, 6857: 2151, 6864: 2152, 6872: 2153, 56035: 2154, 6885: 2155, 6886: 2156, 6890: 2157, 6895: 2158, 6901: 2159, 105208: 2160, 105209: 2161, 105210: 2162, 6906: 2163, 56060: 2164, 105213: 2165, 105214: 2166, 56063: 2167, 105215: 2168, 105217: 2169, 105212: 2170, 105219: 2171, 6916: 2172, 6914: 2173, 105216: 2174, 105218: 2175, 56078: 2176, 6930: 2177, 6933: 2178, 56085: 2179, 6937: 2180, 6940: 2181, 6943: 2182, 6947: 2183, 56107: 2184, 6959: 2185, 6962: 2186, 6973: 2187, 6977: 2188, 6980: 2189, 6984: 2190, 56137: 2191, 6986: 2192, 7008: 2193, 7018: 2194, 7019: 2195, 7022: 2196, 7024: 2197, 7027: 2198, 7028: 2199, 7031: 2200, 7032: 2201, 7041: 2202, 56203: 2203, 56222: 2204, 7070: 2205, 7073: 2206, 7088: 2207, 7095: 2208, 56251: 2209, 56257: 2210, 7106: 2211, 7107: 2212, 7110: 2213, 7111: 2214, 7117: 2215, 7118: 2216, 56271: 2217, 7123: 2218, 7124: 2219, 7140: 2220, 7145: 2221, 7150: 2222, 7153: 2223, 7155: 2224, 56310: 2225, 7161: 2226, 7163: 2227, 7166: 2228, 7172: 2229, 7177: 2230, 56332: 2231, 7183: 2232, 7198: 2233, 7201: 2234, 7212: 2235, 56375: 2236, 56379: 2237, 7233: 2238, 7240: 2239, 7249: 2240, 7277: 2241, 7280: 2242, 7285: 2243, 7286: 2244, 7287: 2245, 7291: 2246, 7292: 2247, 7299: 2248, 7302: 2249, 7308: 2250, 7312: 2251, 7317: 2252, 7320: 2253, 7321: 2254, 7340: 2255, 7344: 2256, 7347: 2257, 7348: 2258, 7349: 2259, 7350: 2260, 7356: 2261, 7371: 2262, 7379: 2263, 7380: 2264, 7389: 2265, 7390: 2266, 7403: 2267, 7404: 2268, 7413: 2269, 56566: 2270, 7422: 2271, 7427: 2272, 56584: 2273, 7437: 2274, 7445: 2275, 7452: 2276, 7453: 2277, 7456: 2278, 7461: 2279, 7462: 2280, 7464: 2281, 7465: 2282, 7467: 2283, 7470: 2284, 7487: 2285, 7488: 2286, 7491: 2287, 7501: 2288, 7509: 2289, 56663: 2290, 56664: 2291, 7516: 2292, 7517: 2293, 7526: 2294, 7527: 2295, 7534: 2296, 7546: 2297, 7566: 2298, 7568: 2299, 7570: 2300, 7575: 2301, 7591: 2302, 7597: 2303, 7607: 2304, 7618: 2305, 7621: 2306, 7626: 2307, 7640: 2308, 7647: 2309, 7661: 2310, 7667: 2311, 7669: 2312, 7670: 2313, 7672: 2314, 7682: 2315, 7685: 2316, 7688: 2317, 7694: 2318, 7696: 2319, 7713: 2320, 7715: 2321, 7724: 2322, 7729: 2323, 7733: 2324, 7742: 2325, 7743: 2326, 7744: 2327, 7752: 2328, 7775: 2329, 56935: 2330, 7784: 2331, 7787: 2332, 7792: 2333, 56950: 2334, 7798: 2335, 7804: 2336, 56964: 2337, 7814: 2338, 7817: 2339, 7818: 2340, 7827: 2341, 7848: 2342, 57001: 2343, 7851: 2344, 7854: 2345, 7862: 2346, 57015: 2347, 7864: 2348, 7866: 2349, 7885: 2350, 7893: 2351, 7896: 2352, 7899: 2353, 7902: 2354, 57058: 2355, 7910: 2356, 7915: 2357, 7922: 2358, 57078: 2359, 57081: 2360, 57085: 2361, 7937: 2362, 7938: 2363, 57090: 2364, 7943: 2365, 7951: 2366, 7955: 2367, 57119: 2368, 7978: 2369, 57134: 2370, 7983: 2371, 7997: 2372, 8010: 2373, 8014: 2374, 8018: 2375, 8030: 2376, 57182: 2377, 8036: 2378, 8040: 2379, 8044: 2380, 57199: 2381, 8060: 2382, 8068: 2383, 8071: 2384, 8081: 2385, 8083: 2386, 57242: 2387, 8093: 2388, 8113: 2389, 8114: 2390, 57268: 2391, 8117: 2392, 8125: 2393, 8133: 2394, 57293: 2395, 8148: 2396, 8154: 2397, 8156: 2398, 8165: 2399, 8167: 2400, 8178: 2401, 8183: 2402, 8190: 2403, 8191: 2404}\n",
      "[13229]\n",
      "tensor([1730])\n",
      "[1722]\n",
      "tensor([565])\n",
      "[56257]\n",
      "tensor([2210])\n"
     ]
    }
   ],
   "source": [
    "## To get the number of true gene and set true gene as data.y label\n",
    "all_true_gene_ids = []\n",
    "\n",
    "for patient in train_pg_subgraph:\n",
    "    all_true_gene_ids.extend(patient.true_gene_ids)\n",
    "\n",
    "for patient in val_pg_subgraph:\n",
    "    all_true_gene_ids.extend(patient.true_gene_ids)\n",
    "    \n",
    "for patient in test_pg_subgraph:\n",
    "    all_true_gene_ids.extend(patient.true_gene_ids)\n",
    "\n",
    "## Get the unique true gene ids\n",
    "unique_true_gene_ids = set(all_true_gene_ids)\n",
    "print(\"the number of unique true gene ids is:\",len(unique_true_gene_ids))\n",
    "\n",
    "## Mapping all unique true gene ids to a index from 0 to the number of unique true gene ids\n",
    "gene_id_mapping = {gene_id: idx for idx, gene_id in enumerate(unique_true_gene_ids)}\n",
    "print(gene_id_mapping)\n",
    "\n",
    "## Add the true gene ids back to the graph\n",
    "for patient in train_pg_subgraph:\n",
    "    patient.y = torch.tensor([gene_id_mapping[gene_id] for gene_id in patient.true_gene_ids], dtype=torch.long)\n",
    "\n",
    "for patient in val_pg_subgraph:\n",
    "    patient.y = torch.tensor([gene_id_mapping[gene_id] for gene_id in patient.true_gene_ids], dtype=torch.long)\n",
    "\n",
    "for patient in test_pg_subgraph:\n",
    "    patient.y = torch.tensor([gene_id_mapping[gene_id] for gene_id in patient.true_gene_ids], dtype=torch.long)\n",
    "\n",
    "print(train_pg_subgraph[0].true_gene_ids)\n",
    "print(train_pg_subgraph[0].y)\n",
    "\n",
    "print(val_pg_subgraph[0].true_gene_ids)\n",
    "print(val_pg_subgraph[0].y)\n",
    "\n",
    "print(test_pg_subgraph[0].true_gene_ids)\n",
    "print(test_pg_subgraph[0].y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocess the trainign data, extract only x, y, edge_index\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "def preprocess_graph_data(dataset):\n",
    "    processed_graphs = []\n",
    "   \n",
    "    for data in dataset:\n",
    "        \n",
    "        new_data = Data(\n",
    "            edge_index=data.edge_index,\n",
    "            y=data.y,\n",
    "            x=data.x,\n",
    "            original_ids = data.original_ids,\n",
    "            edge_attr=data.edge_attr\n",
    "        )\n",
    "        processed_graphs.append(new_data)\n",
    "    \n",
    "    return processed_graphs\n",
    "\n",
    "train_data = preprocess_graph_data(train_pg_subgraph)\n",
    "val_data = preprocess_graph_data(val_pg_subgraph)\n",
    "test_data = preprocess_graph_data(test_pg_subgraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define collate function for handling batched data\n",
    "def optimized_collate_fn(batch):\n",
    "\n",
    "    batch_size = len(batch)\n",
    "    cumsum_nodes = 0\n",
    "    \n",
    "    # Adjust edge indices to account for the node offset in each batch\n",
    "    adjusted_edge_indices = []\n",
    "    for data in batch:\n",
    "        edge_index = data.edge_index + cumsum_nodes\n",
    "        adjusted_edge_indices.append(edge_index)\n",
    "        cumsum_nodes += data.num_nodes\n",
    "\n",
    "    # Concatenate with adjusted indices\n",
    "    x = torch.cat([data.x for data in batch], dim=0)\n",
    "    y = torch.cat([data.y for data in batch], dim=0)\n",
    "    edge_index = torch.cat(adjusted_edge_indices, dim=1)\n",
    "    edge_attr = torch.cat([data.edge_attr for data in batch], dim=0) if batch[0].edge_attr is not None else None\n",
    "    batch_tensor = torch.cat([torch.full((data.num_nodes,), i, dtype=torch.long) for i, data in enumerate(batch)])\n",
    "    # Handle additional attributes\n",
    "    original_ids = torch.cat([torch.tensor(data.original_ids, dtype=torch.long) if isinstance(data.original_ids, list) else data.original_ids for data in batch if data.original_ids is not None])\n",
    "    \n",
    "    return Data(\n",
    "        x = x,\n",
    "        y = y,\n",
    "        edge_index=edge_index,\n",
    "        edge_attr=edge_attr,\n",
    "        batch=batch_tensor,\n",
    "        original_ids=original_ids,\n",
    "        batch_size=batch_size,\n",
    "      \n",
    "    )\n",
    "## torch.dataloader doesn't consider custom data types\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=128, shuffle=True, collate_fn=optimized_collate_fn)\n",
    "val_loader = DataLoader(val_data, batch_size=128, collate_fn=optimized_collate_fn)\n",
    "test_loader = DataLoader(test_data, batch_size=128, collate_fn=optimized_collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class GlobalNodeEmbedding(nn.Module):\n",
    "    def __init__(self, num_global_nodes, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_global_nodes, embedding_dim)\n",
    "        nn.init.xavier_uniform_(self.embedding.weight)\n",
    "    \n",
    "    def forward(self, node_ids):\n",
    "        if isinstance(node_ids, list):\n",
    "            node_ids = torch.tensor(node_ids, dtype=torch.long)\n",
    "        node_ids = node_ids.view(-1)  # Flatten any multi-dimensional input\n",
    "        \n",
    "        # Move to correct device\n",
    "        node_ids = node_ids.to(self.embedding.weight.device)\n",
    "        \n",
    "        embeddings = self.embedding(node_ids)\n",
    "        return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Approximate node degree distribution\n",
    "## Binning node degrees into a soft Histogram\n",
    "## This is used for graph regularization\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Custom SoftHistogram for Graph Regularization\n",
    "class SoftHistogram(nn.Module):\n",
    "    def __init__(self, bins, min, max, sigma):\n",
    "        super().__init__()\n",
    "        self.bins = bins\n",
    "        self.min = min\n",
    "        self.max = max\n",
    "        self.sigma = sigma\n",
    "        self.delta = float(max - min) / float(bins)\n",
    "        self.centers = float(min) + self.delta * (torch.arange(bins).float() + 0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        d = torch.cdist(self.centers[:, None].to(x.device), x[:, None])\n",
    "        x = torch.softmax(-d ** 2 / self.sigma ** 2, dim=0)\n",
    "        x = x.sum(dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import (\n",
    "    GCNConv, GraphConv, SAGEConv, GIN, \n",
    "    global_mean_pool, global_add_pool\n",
    ")\n",
    "\n",
    "## Node-Level Module (F1)\n",
    "class F1NodeLevelModule(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, embedding_dim, conv_type='GCN',\n",
    "                 dropout=0.5, pooling=\"mean\", num_layers=2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Choose GNN Layer Type\n",
    "        if conv_type == \"GCN\":\n",
    "            Conv = GCNConv\n",
    "        elif conv_type == \"Graph\":\n",
    "            Conv = GraphConv\n",
    "        elif conv_type == \"SAGE\":\n",
    "            Conv = SAGEConv\n",
    "        elif conv_type == \"GIN\":\n",
    "            Conv = lambda in_dim, out_dim: GIN(\n",
    "                nn=nn.Sequential(\n",
    "                    nn.Linear(in_dim, out_dim),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(out_dim, out_dim)\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown conv_type: {conv_type}\")\n",
    "\n",
    "        # Define GNN layers\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.bns = nn.ModuleList()\n",
    "\n",
    "        # First Layer\n",
    "        self.convs.append(Conv(input_dim, hidden_dim))\n",
    "        self.bns.append(nn.BatchNorm1d(hidden_dim))\n",
    "\n",
    "        # Hidden Layers\n",
    "        for _ in range(num_layers):\n",
    "            self.convs.append(Conv(hidden_dim, hidden_dim))\n",
    "            self.bns.append(nn.BatchNorm1d(hidden_dim))\n",
    "\n",
    "        # Last Layer\n",
    "        self.convs.append(Conv(hidden_dim, embedding_dim))\n",
    "        self.bns.append(nn.BatchNorm1d(embedding_dim))\n",
    "\n",
    "        self.dropout_layer = nn.Dropout(dropout)\n",
    "        self.pooling = global_mean_pool if pooling == \"mean\" else global_add_pool\n",
    "        \n",
    "        self.norm_graph = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        for conv, bn in zip(self.convs[:-1], self.bns[:-1]):\n",
    "            x = conv(x, edge_index)\n",
    "            x = bn(x)\n",
    "            x = F.relu(x)\n",
    "            x = self.dropout_layer(x)\n",
    "\n",
    "        # Final Layer\n",
    "        x = self.convs[-1](x, edge_index)\n",
    "        x = self.bns[-1](x)\n",
    "        \n",
    "        node_embeddings = x\n",
    "\n",
    "        graph_embeddings = self.pooling(node_embeddings, batch)\n",
    "        \n",
    "        graph_embeddings = (graph_embeddings - graph_embeddings.mean(dim=0)) / (graph_embeddings.std(dim=0) + 1e-8)\n",
    "        graph_embeddings = self.norm_graph(graph_embeddings)\n",
    "        \n",
    "        return node_embeddings, graph_embeddings\n",
    "\n",
    "## Population-Level Module (F2)\n",
    "class F2PopulationLevelGraph(nn.Module):\n",
    "    def __init__(self, embedding_dim, latent_dim, temperature=0.5, threshold=0.05):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        # Latent Space Transformation\n",
    "        self.latent_transform = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, latent_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(latent_dim),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(latent_dim, latent_dim)\n",
    "        )\n",
    "\n",
    "        # Learnable Parameters\n",
    "        self.temperature = nn.Parameter(torch.tensor(temperature, dtype=torch.float32))\n",
    "        self.threshold = nn.Parameter(torch.tensor(threshold, dtype=torch.float32))\n",
    "        self.mu = nn.Parameter(torch.tensor(1.0, dtype=torch.float32))  \n",
    "        self.sigma = nn.Parameter(torch.tensor(1.0, dtype=torch.float32)) \n",
    "\n",
    "    def forward(self, graph_embeddings):\n",
    "        ## Transform to latent space and normalize\n",
    "        latent_space = self.latent_transform(graph_embeddings)\n",
    "        latent_space = F.normalize(latent_space, p=2, dim=-1)\n",
    "\n",
    "        # Compute Pairwise Distances\n",
    "        pairwise_distances = torch.cdist(latent_space, latent_space, p=2)\n",
    "\n",
    "        # Compute Adjacency Matrix with learnable parameters -> get edge_index and edge_weight\n",
    "        adjacency_matrix = torch.sigmoid(-self.temperature * pairwise_distances.pow(2) + self.threshold)\n",
    "\n",
    "        # Create edge_index and edge_weight\n",
    "        edge_index_tuple = torch.nonzero(adjacency_matrix > 0.5).t()\n",
    "        edge_index = edge_index_tuple.long()  # shape: [2, num_edges]\n",
    "        \n",
    "        # Get edge weights from adjacency matrix\n",
    "        edge_weight = adjacency_matrix[edge_index[0], edge_index[1]]\n",
    "\n",
    "        # Compute KL loss\n",
    "        n_nodes = adjacency_matrix.shape[0]\n",
    "        softhist = SoftHistogram(bins=n_nodes, min=0.5, max=n_nodes + 0.5, sigma=0.6)\n",
    "        \n",
    "        # Get degree distribution\n",
    "        degrees = adjacency_matrix.sum(dim=1)  # compute node degrees\n",
    "        degree_dist = softhist(degrees)\n",
    "        degree_dist = degree_dist / degree_dist.sum()  # normalize to get probability distribution\n",
    "        \n",
    "        # Target distribution (can be power law or other specified distribution)\n",
    "        target_dist = self._compute_target_distribution(n_nodes)\n",
    "        \n",
    "        # Compute KL divergence\n",
    "        kl_loss = torch.sum(degree_dist * (torch.log(degree_dist + 1e-8) - torch.log(target_dist + 1e-8)))\n",
    "\n",
    "        return adjacency_matrix, edge_index, edge_weight, kl_loss\n",
    "\n",
    "    def _compute_target_distribution(self, n_nodes):\n",
    "        target_dist = torch.zeros(n_nodes, device=self.temperature.device)\n",
    "        offset = 4 if n_nodes > 4 else 0\n",
    "\n",
    "        # Gaussian distribution centered around mean (mu) with std dev (sigma)\n",
    "        indices = torch.arange(n_nodes - offset, device=self.temperature.device)\n",
    "        target_dist[offset:] = torch.exp(-((indices - self.mu) ** 2) / (2 * self.sigma ** 2))\n",
    "        \n",
    "        return target_dist / target_dist.sum()  # Normalize\n",
    "\n",
    "## Classifier Module (F3)\n",
    "class F3Classifier(nn.Module):\n",
    "    def __init__(self, input_dim_h, gnn_hidden_dim, num_classes, conv_type=\"GCN\",\n",
    "                 gnn_layers=2, dropout=0.2, gene_embedding_dim=64):\n",
    "        super().__init__()\n",
    "        \n",
    "        ## Embedding for true gene id\n",
    "        self.gene_embedding = nn.Embedding(num_classes, gene_embedding_dim)\n",
    "\n",
    "        if conv_type == \"GCN\":\n",
    "            Conv = GCNConv\n",
    "        elif conv_type == \"Graph\":\n",
    "            Conv = GraphConv\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown conv_type: {conv_type}\")\n",
    "\n",
    "        # GNN Layers\n",
    "        self.gnn_layers = nn.ModuleList()\n",
    "        self.gnn_layers.append(Conv(input_dim_h + gene_embedding_dim, gnn_hidden_dim))\n",
    "\n",
    "        for _ in range(gnn_layers - 1):\n",
    "            self.gnn_layers.append(Conv(gnn_hidden_dim, gnn_hidden_dim))\n",
    "\n",
    "        self.batch_norms = nn.ModuleList([nn.BatchNorm1d(gnn_hidden_dim) for _ in range(gnn_layers)])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.ap_transform = nn.Linear(num_classes, num_classes, bias=False)\n",
    "        \n",
    "        ## the final classifier layer\n",
    "        self.classifier = nn.Linear(gnn_hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, h, edge_index, batch, Ap, edge_weight, gene_ids):\n",
    "        gene_embeddings = self.gene_embedding(gene_ids)\n",
    "        gene_embeddings = gene_embeddings[batch]\n",
    "        h = torch.cat([h, gene_embeddings], dim=-1)\n",
    "\n",
    "        ## Some GNN supports edge_weight, some not\n",
    "        for gnn, bn in zip(self.gnn_layers, self.batch_norms):\n",
    "            if edge_weight is not None and isinstance(gnn, (GCNConv, GraphConv)):\n",
    "                h_new = gnn(h, edge_index, edge_weight)\n",
    "            else:\n",
    "                h_new = gnn(h, edge_index)\n",
    "\n",
    "            h_new = bn(h_new)\n",
    "            h_new = self.dropout(h_new)\n",
    "\n",
    "            if h.shape[-1] == h_new.shape[-1]:\n",
    "                h = h + h_new\n",
    "            else:\n",
    "                h = h_new  \n",
    "\n",
    "        # Global Pooling\n",
    "        graph_embeddings = global_mean_pool(h, batch)\n",
    "        batch_size = batch.unique().size(0)\n",
    "        graph_embeddings = graph_embeddings.view(batch_size, -1)\n",
    "\n",
    "        if Ap is not None:\n",
    "            Ap = Ap = F.softmax(Ap, dim=1)\n",
    "            graph_embeddings = torch.matmul(Ap, graph_embeddings)\n",
    "\n",
    "        logits = self.classifier(graph_embeddings)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the GiG model\n",
    "class GiG(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.node_level_module = F1NodeLevelModule(\n",
    "            input_dim=config[\"input_dim\"],\n",
    "            hidden_dim=config[\"hidden_dim\"],\n",
    "            embedding_dim=config[\"embedding_dim\"],\n",
    "            conv_type=config[\"conv_type\"],\n",
    "            dropout=config[\"dropout\"]\n",
    "        )\n",
    "        self.population_level_module = F2PopulationLevelGraph(\n",
    "            embedding_dim=config[\"embedding_dim\"],\n",
    "            latent_dim=config[\"latent_dim\"]\n",
    "        )\n",
    "        self.classifier = F3Classifier(\n",
    "            input_dim_h=config[\"embedding_dim\"],\n",
    "            gnn_hidden_dim=config[\"gnn_hidden_dim\"],\n",
    "            num_classes=config[\"num_classes\"],\n",
    "            conv_type=config[\"conv_type\"],\n",
    "            gnn_layers=config[\"gnn_layers\"],\n",
    "            dropout=config[\"dropout\"]\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        feature_matrix, graph_embeddings = self.node_level_module(data)\n",
    "        adjacency_matrix, edge_index, edge_weight, kl_loss = self.population_level_module(graph_embeddings)\n",
    "        logits = self.classifier(feature_matrix, edge_index, data.batch, adjacency_matrix, edge_weight, data.y)\n",
    "        return logits, adjacency_matrix, kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchmetrics\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingLR\n",
    "\n",
    "# Define Loss Dictionary\n",
    "losses = nn.ModuleDict({\n",
    "    'BCEWithLogitsLoss': nn.BCEWithLogitsLoss(),\n",
    "    'CrossEntropyLoss': nn.CrossEntropyLoss(),\n",
    "    'MultiTaskBCE': nn.BCEWithLogitsLoss(pos_weight=torch.Tensor([10]))\n",
    "})\n",
    "\n",
    "class GiGTrainer(pl.LightningModule):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.save_hyperparameters(config)\n",
    "        self.automatic_optimization = False  # Manual optimization\n",
    "\n",
    "        # Initialize the model\n",
    "        self.model = GiG(config)\n",
    "\n",
    "        # Check and define GlobalNodeEmbedding properly\n",
    "        if \"GlobalNodeEmbedding\" in globals():\n",
    "            self.global_node_embedding = GlobalNodeEmbedding(\n",
    "                num_global_nodes=105220, embedding_dim=config[\"input_dim\"]\n",
    "            )\n",
    "        else:\n",
    "            self.global_node_embedding = nn.Embedding(\n",
    "                num_embeddings=105220, embedding_dim=config[\"input_dim\"]\n",
    "            )\n",
    "\n",
    "        # Set loss function\n",
    "        self.initial_loss = losses[config[\"loss\"]]\n",
    "        self.alpha = config[\"alpha\"]\n",
    "\n",
    "        # Store embeddings for debugging\n",
    "        self.node_embeddings = None\n",
    "\n",
    "    def forward(self, data):\n",
    "        return self.model(data)\n",
    "\n",
    "    def _shared_step(self, data, addition):\n",
    "        \"\"\"Common logic for train, validation, and test steps.\"\"\"\n",
    "        # Ensure input embeddings are used correctly\n",
    "        data.x = self.global_node_embedding(data.original_ids.long().to(self.device))\n",
    "\n",
    "        # Forward pass\n",
    "        logits, adj_matrix, kl_loss = self.model(data)\n",
    "\n",
    "        # Prepare labels\n",
    "        labels = data.y.view(-1).long().to(self.device)\n",
    "\n",
    "        # Compute classification loss\n",
    "        classification_loss = self.initial_loss(logits, labels)\n",
    "\n",
    "        # Compute KL Loss scaling factor\n",
    "        kl_weight = self.alpha * (self.current_epoch / (self.trainer.max_epochs + 1e-8))  # Prevent division by zero\n",
    "\n",
    "        # Total loss\n",
    "        total_loss = classification_loss + kl_weight * kl_loss\n",
    "\n",
    "        # Compute metrics\n",
    "        acc = torchmetrics.functional.accuracy(\n",
    "            logits.argmax(dim=-1), labels, task=\"multiclass\", num_classes=logits.shape[1]\n",
    "        )\n",
    "        f1 = torchmetrics.functional.f1_score(\n",
    "            logits.argmax(dim=-1), labels, task=\"multiclass\", num_classes=logits.shape[1]\n",
    "        )\n",
    "\n",
    "        # Log metrics\n",
    "        metrics = {\n",
    "            f\"{addition}_acc\": acc,\n",
    "            f\"{addition}_f1\": f1,\n",
    "            f\"{addition}_loss\": total_loss,\n",
    "            f\"{addition}_classification_loss\": classification_loss,\n",
    "            f\"{addition}_kl_loss\": kl_loss\n",
    "        }\n",
    "\n",
    "        return metrics, total_loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        metrics, loss = self._shared_step(batch, \"train\")\n",
    "        self.log_dict(metrics, prog_bar=True, batch_size=len(batch.y))\n",
    "        \n",
    "        return loss  # Enable Lightning auto-optimization\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # Extract node embeddings\n",
    "        node_embeddings, graph_embeddings = self.model.node_level_module(batch)\n",
    "        node_embeddings = node_embeddings.detach().cpu()\n",
    "\n",
    "        # Store embeddings for later analysis\n",
    "        if self.node_embeddings is None:\n",
    "            self.node_embeddings = node_embeddings\n",
    "        else:\n",
    "            self.node_embeddings = torch.cat([self.node_embeddings, node_embeddings], dim=0)\n",
    "\n",
    "        # Compute validation metrics\n",
    "        metrics, _ = self._shared_step(batch, \"val\")\n",
    "        self.log_dict(metrics, prog_bar=True, batch_size=len(batch.y))\n",
    "        return metrics\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        metrics, _ = self._shared_step(batch, \"test\")\n",
    "        self.log_dict(metrics, batch_size=len(batch.y))\n",
    "        return metrics\n",
    "\n",
    "    def on_test_end(self):\n",
    "        \"\"\"Save and analyze embeddings after testing.\"\"\"\n",
    "        if self.node_embeddings is not None:\n",
    "            print(\"Saving node embeddings...\")\n",
    "            torch.save(self.node_embeddings, \"node_embeddings.pth\")\n",
    "            print(\"Saved node embeddings shape:\", self.node_embeddings.shape)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"Set up optimizers and learning rate schedulers.\"\"\"\n",
    "        # Main parameters excluding population graph learnable scalars\n",
    "        main_params = [\n",
    "            param for name_, param in self.model.population_level_module.named_parameters()\n",
    "            if name_ not in [\"temperature\", \"threshold\", \"mu\", \"sigma\"]\n",
    "        ]\n",
    "        main_params.extend(self.model.node_level_module.parameters())\n",
    "        main_params.extend(self.model.classifier.parameters())\n",
    "\n",
    "        # Define optimizers\n",
    "        main_optimizer = torch.optim.Adam(main_params, lr=self.config[\"lr\"])\n",
    "        lgl_optimizer = torch.optim.Adam([\n",
    "            self.model.population_level_module.temperature,\n",
    "            self.model.population_level_module.threshold,\n",
    "        ], lr=self.config[\"lr_theta_temp\"])\n",
    "\n",
    "        # Define learning rate scheduler\n",
    "        if self.config[\"scheduler\"] == \"ReduceLROnPlateau\":\n",
    "            scheduler_dict = {\n",
    "                \"scheduler\": ReduceLROnPlateau(\n",
    "                    main_optimizer, mode=\"min\", patience=10, threshold=0.0001, verbose=True\n",
    "                ),\n",
    "                \"interval\": \"epoch\",\n",
    "                \"monitor\": \"val_loss\",\n",
    "                \"frequency\": 1\n",
    "            }\n",
    "        else:\n",
    "            scheduler_dict = {\n",
    "                \"scheduler\": CosineAnnealingLR(main_optimizer, T_max=10),\n",
    "                \"interval\": \"epoch\",\n",
    "                \"monitor\": \"val_loss\",\n",
    "                \"frequency\": 1\n",
    "            }\n",
    "\n",
    "        return [main_optimizer, lgl_optimizer], [scheduler_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/kai/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                  | Type                | Params | Mode \n",
      "----------------------------------------------------------------------\n",
      "0 | model                 | GiG                 | 6.1 M  | train\n",
      "1 | global_node_embedding | GlobalNodeEmbedding | 736 K  | train\n",
      "2 | initial_loss          | CrossEntropyLoss    | 0      | train\n",
      "----------------------------------------------------------------------\n",
      "6.9 M     Trainable params\n",
      "0         Non-trainable params\n",
      "6.9 M     Total params\n",
      "27.441    Total estimated model params size (MB)\n",
      "47        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a58e385c207b46c4a3a78477a79cb44a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "linear(): argument 'input' (position 1) must be Tensor, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 66\u001b[0m\n\u001b[1;32m     54\u001b[0m model \u001b[38;5;241m=\u001b[39m GiGTrainer(config)\n\u001b[1;32m     56\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     57\u001b[0m     max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[1;32m     58\u001b[0m     accelerator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpu\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     63\u001b[0m     benchmark\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     64\u001b[0m )\n\u001b[0;32m---> 66\u001b[0m trainer\u001b[38;5;241m.\u001b[39mfit(model, train_loader, val_loader)\n\u001b[1;32m     67\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtest(model, test_loader)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:539\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 539\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_and_handle_interrupt(\n\u001b[1;32m    540\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    541\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:47\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trainer_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     50\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:575\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    569\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    571\u001b[0m     ckpt_path,\n\u001b[1;32m    572\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    573\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    574\u001b[0m )\n\u001b[0;32m--> 575\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run(model, ckpt_path\u001b[38;5;241m=\u001b[39mckpt_path)\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    578\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:982\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    977\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    979\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    981\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 982\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_stage()\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    986\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    987\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:1024\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1022\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m   1023\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n\u001b[0;32m-> 1024\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_sanity_check()\n\u001b[1;32m   1025\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[1;32m   1026\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mrun()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:1053\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1050\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_start\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;66;03m# run eval step\u001b[39;00m\n\u001b[0;32m-> 1053\u001b[0m val_loop\u001b[38;5;241m.\u001b[39mrun()\n\u001b[1;32m   1055\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1057\u001b[0m \u001b[38;5;66;03m# reset logger connector\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:179\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    177\u001b[0m     context_manager \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mno_grad\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loop_run(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/evaluation_loop.py:144\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mis_last_batch \u001b[38;5;241m=\u001b[39m data_fetcher\u001b[38;5;241m.\u001b[39mdone\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;66;03m# run step hooks\u001b[39;00m\n\u001b[0;32m--> 144\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;66;03m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/evaluation_loop.py:433\u001b[0m, in \u001b[0;36m_EvaluationLoop._evaluation_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001b[0m\n\u001b[1;32m    427\u001b[0m hook_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_step\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mtesting \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    428\u001b[0m step_args \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    429\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_step_args_from_hook_kwargs(hook_kwargs, hook_name)\n\u001b[1;32m    430\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_dataloader_iter\n\u001b[1;32m    431\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m (dataloader_iter,)\n\u001b[1;32m    432\u001b[0m )\n\u001b[0;32m--> 433\u001b[0m output \u001b[38;5;241m=\u001b[39m call\u001b[38;5;241m.\u001b[39m_call_strategy_hook(trainer, hook_name, \u001b[38;5;241m*\u001b[39mstep_args)\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_processed()\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m using_dataloader_iter:\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;66;03m# update the hook kwargs now that the step method might have consumed the iterator\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:323\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 323\u001b[0m     output \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    326\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/strategies/strategy.py:412\u001b[0m, in \u001b[0;36mStrategy.validation_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module:\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_redirection(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 412\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mvalidation_step(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[0;32mIn[32], line 101\u001b[0m, in \u001b[0;36mGiGTrainer.validation_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_embeddings \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_embeddings, node_embeddings], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# Compute validation metrics\u001b[39;00m\n\u001b[0;32m--> 101\u001b[0m metrics, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shared_step(batch, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_dict(metrics, prog_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(batch\u001b[38;5;241m.\u001b[39my))\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m metrics\n",
      "Cell \u001b[0;32mIn[32], line 50\u001b[0m, in \u001b[0;36mGiGTrainer._shared_step\u001b[0;34m(self, data, addition)\u001b[0m\n\u001b[1;32m     47\u001b[0m data\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglobal_node_embedding(data\u001b[38;5;241m.\u001b[39moriginal_ids\u001b[38;5;241m.\u001b[39mlong()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice))\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m logits, adj_matrix, kl_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(data)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Prepare labels\u001b[39;00m\n\u001b[1;32m     53\u001b[0m labels \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39my\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mlong()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[31], line 30\u001b[0m, in \u001b[0;36mGiG.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     27\u001b[0m feature_matrix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_level_module(data)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Get the population graph outputs\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m x, edge_index, edge_weight, adj_matrix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpopulation_level_module(feature_matrix)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Debugging Output\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=== Population-Level Graph (F2) Output ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[26], line 106\u001b[0m, in \u001b[0;36mF2PopulationLevelGraph.forward\u001b[0;34m(self, graph_embeddings)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, graph_embeddings):\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;66;03m## Transform to latent space and normalize\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     latent_space \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatent_transform(graph_embeddings)\n\u001b[1;32m    107\u001b[0m     latent_space \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnormalize(latent_space, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;66;03m# Compute Pairwise Distances\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[0;31mTypeError\u001b[0m: linear(): argument 'input' (position 1) must be Tensor, not tuple"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import (\n",
    "    ModelCheckpoint,\n",
    "    EarlyStopping,\n",
    "    LearningRateMonitor\n",
    ")\n",
    "import wandb\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "\n",
    "# Setup callbacks\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        monitor='val_loss',\n",
    "        dirpath='checkpoints',\n",
    "        filename='gig-{epoch:02d}-{val_loss:.2f}',\n",
    "        save_top_k=3,\n",
    "        mode='min',\n",
    "    ),\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=3,\n",
    "        mode='min',\n",
    "        verbose=True\n",
    "    ),\n",
    "    LearningRateMonitor(logging_interval='epoch')\n",
    "]\n",
    "\n",
    "# Setup logger\n",
    "wandb_logger = WandbLogger(project=\"gig-model\")\n",
    "\n",
    "config = {\n",
    "    \"input_dim\": 7,\n",
    "    \"hidden_dim\": 64,\n",
    "    \"embedding_dim\": 64,\n",
    "    \"latent_dim\": 32,\n",
    "    \"gnn_hidden_dim\": 64,\n",
    "    \"num_classes\": len(unique_true_gene_ids),\n",
    "    \n",
    "    \"conv_type\": \"GCN\",\n",
    "    \"gnn_layers\": 2,\n",
    "    \"dropout\": 0.5,\n",
    "    \"lr\": 0.01,  # Main learning rate\n",
    "    \"optimizer_lr\": 0.001,\n",
    "    \"lr_theta_temp\": 0.005, \n",
    "    \"alpha\": 0.00001,\n",
    "   \n",
    "    \"loss\": \"CrossEntropyLoss\",\n",
    "    \"scheduler\": \"ReduceLROnPlateau\"\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "model = GiGTrainer(config)\n",
    "\n",
    "trainer = Trainer(\n",
    "    max_epochs=5,\n",
    "    accelerator='gpu',\n",
    "    devices='auto',\n",
    "    callbacks=callbacks,\n",
    "    logger=wandb_logger,\n",
    "    deterministic=False,\n",
    "    benchmark=True\n",
    ")\n",
    "\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "trainer.test(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import networkx as nx\n",
    "\n",
    "# # Assuming test_nx_subgraph is a list of graphs\n",
    "# subgraph = train_nx_subgraph[3]  # First subgraph\n",
    "\n",
    "# # Check if the graph is directed\n",
    "# is_directed = subgraph.is_directed()\n",
    "\n",
    "# # Check if the graph is connected\n",
    "# # For directed graphs, check strongly connected or weakly connected\n",
    "# if is_directed:\n",
    "#     is_connected = nx.is_strongly_connected(subgraph)  # Strongly connected\n",
    "#     is_weakly_connected = nx.is_weakly_connected(subgraph)  # Weakly connected\n",
    "#     print(f\"Graph is directed: {is_directed}\")\n",
    "#     print(f\"Graph is strongly connected: {is_connected}\")\n",
    "#     print(f\"Graph is weakly connected: {is_weakly_connected}\")\n",
    "# else:\n",
    "#     is_connected = nx.is_connected(subgraph)  # Connected (undirected case)\n",
    "#     print(f\"Graph is directed: {is_directed}\")\n",
    "#     print(f\"Graph is connected: {is_connected}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create the output directory if it doesn't exist\n",
    "# output_folder = './Graph Outputs'\n",
    "# os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# # Combined lists to save\n",
    "# lists_to_save = {\n",
    "#     'val_nx_subgraph': val_nx_subgraph,\n",
    "#     'val_pg_subgraph': val_pg_subgraph,\n",
    "\n",
    "#     'train_nx_subgraph': train_nx_subgraph,\n",
    "#     'train_pg_subgraph': train_pg_subgraph,\n",
    "\n",
    "#     'test_nx_subgraph': test_nx_subgraph,\n",
    "#     'test_pg_subgraph': test_pg_subgraph,\n",
    "# }\n",
    "\n",
    "# # Save each list using pickle\n",
    "# for list_name, list_data in lists_to_save.items():\n",
    "#     file_path = os.path.join(output_folder, f'{list_name}.pkl')\n",
    "#     with open(file_path, 'wb') as file:\n",
    "#         pickle.dump(list_data, file)\n",
    "#     print(f'Saved {list_name} to {file_path}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
